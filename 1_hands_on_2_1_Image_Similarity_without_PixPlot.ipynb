{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment: Attractors in Image Space\n",
        "\n",
        "If we generate say 100 images an image generator like https://perchance.org/ai-text-to-image-generator and use the same prompt each time and we try the '[datafication of a kiss](https://www.cyberneticforests.com/news/how-to-read-an-ai-image)' approach[^1], what can we see? The idea is to treat the generated images as an infographic of the underlying dataset. What visual tropes or elements seem to be in play in the different clusters? What does that imply about the underlying data?\n",
        "\n",
        "1. Go to Perchance and generate multiple images from the same prompt. use the same prompt 11 times. Take a screenshot of the grid of results each time. Rename your screenshots run1.png, run2.png, run3.png, run4.png, run5.png etc.  Then, run the code below in **PART ONE**. Drag and drop the screenshots into the file tray here, in the `input` folder you make. Then slice each screenshot into its constituent sub images.\n",
        "2. **PART TWO** measures the similarity of each image by creating an embedding or vectorized representation of the images and seeing how close each pair of vectors are in turn. It then visualizes the results for us.\n",
        "3. **PART THREE** Visualizes the results. Not as pretty as PixPlot, but it all works."
      ],
      "metadata": {
        "id": "etXjjCw3xe_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part One"
      ],
      "metadata": {
        "id": "-xRy74e4x_c5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOZiY_AWxV5X"
      },
      "outputs": [],
      "source": [
        "## run this\n",
        "#!rm -r input #if you're processing different images w/ different aspect ratios, do it in batches. Do one batch, then uncomment this line to get a fresh input folder. Then change column/rows as appropriate, below\n",
        "!mkdir input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you have a folder of zipped images ready to go\n",
        "# drag and drop the zipped folder into the file tray (hit the the folder icon to expand it if necessary) at left\n",
        "# then adjust this code to use your file name, and run it:\n",
        "!unzip my_images.zip -d input"
      ],
      "metadata": {
        "id": "Y_4-OinrxyFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create some necessary functions for manipulating grids of images\n",
        "\n",
        "The functions below will cut your 'contact' sheet of multiple images up into single images. We want to feed one image at a time to pixplot for visualization, so we need to split the grid into separate images. You run the cell that `def`ines the function, then we run the function on your images in the subsequent block."
      ],
      "metadata": {
        "id": "V61PzJgdx1pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import math\n",
        "import logging\n",
        "\n",
        "def setup_logging():\n",
        "    \"\"\"Configure logging to track image processing details.\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO,\n",
        "                       format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "def validate_dimensions(img_width, img_height, num_columns, num_rows):\n",
        "    \"\"\"\n",
        "    Validate that the image can be evenly divided into the specified grid.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (is_valid, single_width, single_height, warning_message)\n",
        "    \"\"\"\n",
        "    single_width = img_width / num_columns\n",
        "    single_height = img_height / num_rows\n",
        "\n",
        "    # Check if dimensions result in whole numbers\n",
        "    is_width_whole = single_width.is_integer()\n",
        "    is_height_whole = single_height.is_integer()\n",
        "\n",
        "    warning_msg = \"\"\n",
        "    if not (is_width_whole and is_height_whole):\n",
        "        warning_msg = (\n",
        "            f\"Warning: Image dimensions ({img_width}x{img_height}) \"\n",
        "            f\"cannot be evenly divided into {num_columns}x{num_rows} grid. \"\n",
        "            f\"Subimages will be {single_width:.2f}x{single_height:.2f} pixels.\"\n",
        "        )\n",
        "\n",
        "    return (is_width_whole and is_height_whole,\n",
        "            int(single_width),\n",
        "            int(single_height),\n",
        "            warning_msg)\n",
        "\n",
        "def slice_image(image_path, output_dir, num_columns=3, num_rows=2, strict_mode=True):\n",
        "    \"\"\"\n",
        "    Slice an image into a grid of smaller images with validation and logging.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image\n",
        "        output_dir (str): Directory to save the output images\n",
        "        num_columns (int): Number of columns in the grid\n",
        "        num_rows (int): Number of rows in the grid\n",
        "        strict_mode (bool): If True, raises error on uneven divisions\n",
        "\n",
        "    Returns:\n",
        "        list: List of paths to the generated images\n",
        "    \"\"\"\n",
        "    logger = setup_logging()\n",
        "\n",
        "    # Load the image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_width, img_height = img.size\n",
        "\n",
        "    logger.info(f\"Processing image: {image_path}\")\n",
        "    logger.info(f\"Original dimensions: {img_width}x{img_height}\")\n",
        "\n",
        "    # Validate dimensions\n",
        "    is_valid, single_width, single_height, warning_msg = validate_dimensions(\n",
        "        img_width, img_height, num_columns, num_rows\n",
        "    )\n",
        "\n",
        "    if warning_msg:\n",
        "        logger.warning(warning_msg)\n",
        "        if strict_mode:\n",
        "            raise ValueError(\"Image dimensions must be exactly divisible in strict mode\")\n",
        "\n",
        "    # Ensure we're working with integer dimensions\n",
        "    single_width = math.floor(single_width)\n",
        "    single_height = math.floor(single_height)\n",
        "\n",
        "    logger.info(f\"Subimage dimensions: {single_width}x{single_height}\")\n",
        "\n",
        "    output_paths = []\n",
        "    for row in range(num_rows):\n",
        "        for col in range(num_columns):\n",
        "            left = col * single_width\n",
        "            upper = row * single_height\n",
        "            right = left + single_width\n",
        "            lower = upper + single_height\n",
        "\n",
        "            # Create a new blank image instead of cropping\n",
        "            cropped_img = Image.new('RGB', (single_width, single_height))\n",
        "            # Copy the exact region we want\n",
        "            region = img.crop((left, upper, right, lower))\n",
        "            cropped_img.paste(region, (0, 0))\n",
        "\n",
        "            # Strip any existing metadata\n",
        "            data = list(cropped_img.getdata())\n",
        "            clean_img = Image.new('RGB', cropped_img.size)\n",
        "            clean_img.putdata(data)\n",
        "\n",
        "            base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "            original_ext = os.path.splitext(image_path)[1].lower()\n",
        "            output_path = os.path.join(\n",
        "                output_dir,\n",
        "                f'{base_name}_cropped_{row * num_columns + col + 1}{original_ext}'\n",
        "            )\n",
        "\n",
        "            # Save with explicit dimensions\n",
        "            clean_img.save(output_path, format=original_ext[1:])\n",
        "\n",
        "            logger.info(f\"Saved subimage: {output_path}\")\n",
        "            output_paths.append(output_path)\n",
        "\n",
        "    return output_paths\n",
        "\n",
        "def process_images(input_dir=\"input\", output_dir=\"all_images\",\n",
        "                  num_columns=3, num_rows=2, strict_mode=True):\n",
        "    \"\"\"\n",
        "    Process all supported image files in the input directory with validation.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Input directory containing images\n",
        "        output_dir (str): Directory to save the output images\n",
        "        num_columns (int): Number of columns in the grid\n",
        "        num_rows (int): Number of rows in the grid\n",
        "        strict_mode (bool): If True, raises error on uneven divisions\n",
        "    \"\"\"\n",
        "    logger = setup_logging()\n",
        "\n",
        "    SUPPORTED_FORMATS = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    processed_files = 0\n",
        "    errors = 0\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith(SUPPORTED_FORMATS):\n",
        "            image_path = os.path.join(input_dir, filename)\n",
        "            try:\n",
        "                slice_image(image_path, output_dir, num_columns, num_rows, strict_mode)\n",
        "                processed_files += 1\n",
        "                logger.info(f\"Successfully processed: {filename}\")\n",
        "            except Exception as e:\n",
        "                errors += 1\n",
        "                logger.error(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    logger.info(f\"Processing complete. {processed_files} images processed, {errors} errors.\")\n",
        "    logger.info(f\"Subimages saved to '{output_dir}' directory.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pid4keMfx5GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## slice the images\n",
        "If your image generator generates previews in a grid-like format, you can take a screenshot of that grid and then run 'process_images' below to cut them into individual images. Just change the number of columns and rows appropriately Eg, craiyon gives you 3x3 preview images;  https://perchance.org/ai-text-to-image-generator with 'casual photography', 6 photos, portrait, will return 3 x 2.\n",
        "\n",
        "For instance, here's a 'contact' sheet I made with Perchance with the prompt, `an archaeologist at work`; you'd set the code in the next block to have 6 columns and 3 rows.\n",
        "\n",
        "![](https://github.com/shawngraham/homecooked-history/blob/main/genai-images-as-infographics/perchance-archaeologists.png?raw=true)"
      ],
      "metadata": {
        "id": "4vSimEHwyESx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_images(\n",
        "    input_dir=\"input\",\n",
        "    output_dir=\"all_images\",\n",
        "    num_columns=6,  #make sure you set this correctly!\n",
        "    num_rows=3,  #make sure you set this correctly!\n",
        "    strict_mode=False\n",
        ")\n",
        "\n",
        "# For strict validation (will raise error if dimensions don't divide evenly)\n",
        "#process_images(strict_mode=True)\n",
        "\n",
        "# For more lenient processing (will warn but continue)\n",
        "#process_images(strict_mode=False)"
      ],
      "metadata": {
        "id": "UKFUSDv1yHet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the block runs, check your file browser for 'all_images'. You should see a number of image files in there (you can double-click them to see if everything worked correctly)."
      ],
      "metadata": {
        "id": "4iUFgqdhyJ5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part Two"
      ],
      "metadata": {
        "id": "oHfBJ2IGyNPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "# ie, everytime the computer needs to calculate a random number, it'll start by using the\n",
        "# numbers we set here and this will make all subsequent 'random' calculations the same ones\n",
        "# each time. You thought computers were actually random? Nope. They always take a seed value\n",
        "# but most of the time we don't set it, so it always appears random-enough to us. Just fyi.\n",
        "torch.manual_seed(24)\n",
        "np.random.seed(24)\n",
        "random.seed(24)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "CQ5EyxKPyRQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data directory\n",
        "DATA_DIR = \"all_images\"\n",
        "\n",
        "def load_image_paths(data_dir):\n",
        "    \"\"\"Load all image paths from the data directory and its subdirectories.\"\"\"\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif'}\n",
        "    image_paths = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "# Load all image paths\n",
        "image_paths = load_image_paths(DATA_DIR)\n",
        "print(f\"Found {len(image_paths)} images in the dataset\")\n",
        "\n",
        "# Display some example images\n",
        "def display_sample_images(image_paths, n_samples=6):\n",
        "    \"\"\"Display a sample of images from the dataset.\"\"\"\n",
        "    if len(image_paths) == 0:\n",
        "        print(\"No images found in the dataset!\")\n",
        "        return\n",
        "\n",
        "    sample_paths = random.sample(image_paths, min(n_samples, len(image_paths)))\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, path in enumerate(sample_paths):\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].set_title(os.path.basename(path))\n",
        "            axes[i].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display sample images\n",
        "display_sample_images(image_paths)"
      ],
      "metadata": {
        "id": "F7Z39MQvySu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extractor\n",
        "Now the image model we're going to use has learned already all sorts of labels. We don't want that; we just want its understanding of shapes and form. So we're going to get rid of that last layer and we'll end up with just the vectors that describe images."
      ],
      "metadata": {
        "id": "Bz3BiXzqyawk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use ResNet-50, a popular convolutional neural network architecture.\n",
        "# We'll remove the final classification layer to get feature embeddings.\n",
        "\n",
        "class ImageFeatureExtractor:\n",
        "    def __init__(self, model_name='resnet50'):\n",
        "        \"\"\"Initialize the feature extractor with a pre-trained model.\"\"\"\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load pre-trained ResNet-50\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Remove the final classification layer\n",
        "        # This gives us 2048-dimensional feature vectors\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        self.model.eval()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Define image preprocessing transforms\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def extract_features(self, image_path):\n",
        "        \"\"\"Extract features from a single image.\"\"\"\n",
        "        try:\n",
        "            # Load and preprocess the image\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                features = self.model(input_tensor)\n",
        "\n",
        "            # Flatten the features\n",
        "            features = features.view(features.size(0), -1)\n",
        "\n",
        "            return features.cpu().numpy().flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "# Initialize the feature extractor\n",
        "extractor = ImageFeatureExtractor()\n",
        "print(\"Feature extractor initialized successfully!\")"
      ],
      "metadata": {
        "id": "nNFORuCxycRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop our pictures through this model\n",
        "\n",
        "Notice that the information about the image categories implied by the folder name they're in is not used in any way for turning the images into vectors. Later on we'll visualize where in the embedding space the different images fall, and we'll colour the dots by their original categories: which means that you can get a sense of how good those category images might actually be..."
      ],
      "metadata": {
        "id": "Et_tfZzeyeqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This step processes all images and extracts their feature embeddings.\n",
        "# Note: This might take a while depending on the number of images.\n",
        "\n",
        "def extract_all_features(image_paths, extractor):\n",
        "    \"\"\"Extract features from all images in the dataset.\"\"\"\n",
        "    features = []\n",
        "    valid_paths = []\n",
        "\n",
        "    print(\"Extracting features from all images...\")\n",
        "\n",
        "    for path in tqdm(image_paths, desc=\"Processing images\"):\n",
        "        feature = extractor.extract_features(path)\n",
        "        if feature is not None:\n",
        "            features.append(feature)\n",
        "            valid_paths.append(path)\n",
        "\n",
        "    features = np.array(features)\n",
        "    print(f\"Successfully extracted features from {len(features)} images\")\n",
        "    print(f\"Feature shape: {features.shape}\")\n",
        "\n",
        "    return features, valid_paths\n",
        "\n",
        "# Extract features (this might take a few minutes)\n",
        "if len(image_paths) > 0:\n",
        "    features, valid_image_paths = extract_all_features(image_paths, extractor)\n",
        "else:\n",
        "    print(\"No images found to process!\")\n",
        "    features, valid_image_paths = np.array([]), []"
      ],
      "metadata": {
        "id": "E6qkjedGygiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Similarity!\n",
        "\n",
        "Because vectors are directions, we can work out similarity by doing some geometry on them. In this case we'll use cosine similarity."
      ],
      "metadata": {
        "id": "MNHIcDMlyjMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we'll compute similarity between images using cosine similarity.\n",
        "# Cosine similarity measures the cosine of the angle between two vectors.\n",
        "def find_similar_images(query_idx, features, valid_paths, top_k=5):\n",
        "    \"\"\"Find the most similar images to a query image.\"\"\"\n",
        "    if len(features) == 0:\n",
        "        return [], []\n",
        "\n",
        "    # Compute cosine similarity between query and all images\n",
        "    query_features = features[query_idx:query_idx+1]\n",
        "    similarities = cosine_similarity(query_features, features).flatten()\n",
        "\n",
        "    # Get top-k most similar images (excluding the query itself)\n",
        "    similar_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    # Remove the query image itself from results\n",
        "    similar_indices = similar_indices[similar_indices != query_idx][:top_k]\n",
        "\n",
        "    return similar_indices, similarities[similar_indices]\n",
        "\n",
        "def display_similar_images(query_idx, similar_indices, similarities, valid_paths):\n",
        "    \"\"\"Display the query image and its most similar images.\"\"\"\n",
        "    if len(valid_paths) == 0:\n",
        "        print(\"No valid images to display!\")\n",
        "        return\n",
        "\n",
        "    n_images = len(similar_indices) + 1\n",
        "    fig, axes = plt.subplots(1, n_images, figsize=(15, 3))\n",
        "\n",
        "    if n_images == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    # Display query image\n",
        "    query_img = Image.open(valid_paths[query_idx])\n",
        "    axes[0].imshow(query_img)\n",
        "    axes[0].set_title(f\"Query Image\\n{os.path.basename(valid_paths[query_idx])}\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Display similar images\n",
        "    for i, (idx, sim) in enumerate(zip(similar_indices, similarities)):\n",
        "        similar_img = Image.open(valid_paths[idx])\n",
        "        axes[i+1].imshow(similar_img)\n",
        "        axes[i+1].set_title(f\"Similarity: {sim:.3f}\\n{os.path.basename(valid_paths[idx])}\")\n",
        "        axes[i+1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4__n-x5kylFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar images to a random query\n",
        "if len(features) > 0:\n",
        "    query_idx = random.randint(0, len(features) - 1)\n",
        "    similar_indices, similarities = find_similar_images(query_idx, features, valid_image_paths)\n",
        "\n",
        "    print(f\"Finding images similar to: {os.path.basename(valid_image_paths[query_idx])}\")\n",
        "    display_similar_images(query_idx, similar_indices, similarities, valid_image_paths)\n",
        "else:\n",
        "    print(\"No features available for similarity computation!\")"
      ],
      "metadata": {
        "id": "WyIu5ztAyngZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def another_similarity_search(features, valid_paths, n_queries=3):\n",
        "    \"\"\"Demonstrate similarity search with multiple random queries.\"\"\"\n",
        "    if len(features) == 0:\n",
        "        print(\"No features available for similarity search!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Demonstrating similarity search with {n_queries} random queries...\")\n",
        "\n",
        "    for i in range(n_queries):\n",
        "        print(f\"\\n--- Query {i+1} ---\")\n",
        "        query_idx = random.randint(0, len(features) - 1)\n",
        "        similar_indices, similarities = find_similar_images(query_idx, features, valid_paths, top_k=4)\n",
        "        display_similar_images(query_idx, similar_indices, similarities, valid_paths)\n",
        "\n",
        "# Run another similarity search\n",
        "if len(features) > 0:\n",
        "    another_similarity_search(features, valid_image_paths)"
      ],
      "metadata": {
        "id": "Zur4_x5iyoJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part THREE\n",
        "Visualize Similarity!"
      ],
      "metadata": {
        "id": "-jidKCk_yqRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# High-dimensional embeddings are hard to visualize directly.\n",
        "# We'll use dimensionality reduction techniques to project them into 2D space.\n",
        "\n",
        "def visualize_embeddings_2d(features, valid_paths, method='tsne', n_samples=None,\n",
        "                           show_labels=False):\n",
        "    \"\"\"\n",
        "    Visualize embeddings in 2D space using PCA or t-SNE.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    features : numpy.ndarray\n",
        "        Feature embeddings for all images\n",
        "    valid_paths : list\n",
        "        List of valid image paths\n",
        "    method : str\n",
        "        Dimensionality reduction method ('pca' or 'tsne')\n",
        "    n_samples : int, optional\n",
        "        Number of samples to visualize (None for all)\n",
        "    show_labels : bool\n",
        "        Whether to show filename labels on the plot\n",
        "    \"\"\"\n",
        "    if len(features) == 0:\n",
        "        print(\"No features available for visualization!\")\n",
        "        return\n",
        "\n",
        "    # Limit number of samples for faster computation\n",
        "    if n_samples and len(features) > n_samples:\n",
        "        indices = random.sample(range(len(features)), n_samples)\n",
        "        features_subset = features[indices]\n",
        "        paths_subset = [valid_paths[i] for i in indices]\n",
        "    else:\n",
        "        features_subset = features\n",
        "        paths_subset = valid_paths\n",
        "\n",
        "    print(f\"Visualizing {len(features_subset)} images using {method.upper()}...\")\n",
        "\n",
        "    if method == 'pca':\n",
        "        # Principal Component Analysis\n",
        "        reducer = PCA(n_components=2, random_state=42)\n",
        "        embeddings_2d = reducer.fit_transform(features_subset)\n",
        "        title = f\"Image Embeddings Visualization (PCA)\\nExplained Variance: {reducer.explained_variance_ratio_.sum():.2%}\"\n",
        "\n",
        "    elif method == 'tsne':\n",
        "        # t-Distributed Stochastic Neighbor Embedding\n",
        "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features_subset)-1))\n",
        "        embeddings_2d = reducer.fit_transform(features_subset)\n",
        "        title = \"Image Embeddings Visualization (t-SNE)\"\n",
        "\n",
        "    # Create the visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Color points by their directory (if available)\n",
        "    colors = []\n",
        "    labels = []\n",
        "    for path in paths_subset:\n",
        "        # Extract directory name as label\n",
        "        parent_dir = os.path.basename(os.path.dirname(path))\n",
        "        if parent_dir not in labels:\n",
        "            labels.append(parent_dir)\n",
        "        colors.append(labels.index(parent_dir))\n",
        "\n",
        "    # Create scatter plot\n",
        "    scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "                       c=colors, cmap='tab10', alpha=0.7, s=50)\n",
        "\n",
        "    # Add filename labels if requested\n",
        "    if show_labels:\n",
        "        for i, (x, y, path) in enumerate(zip(embeddings_2d[:, 0], embeddings_2d[:, 1], paths_subset)):\n",
        "            filename = os.path.basename(path)\n",
        "            # Truncate long filenames\n",
        "            if len(filename) > 15:\n",
        "                filename = filename[:12] + '...'\n",
        "            ax.annotate(filename, (x, y), xytext=(5, 5), textcoords='offset points',\n",
        "                       fontsize=8, alpha=0.7, ha='left')\n",
        "\n",
        "    # Add colorbar if we have multiple categories\n",
        "    # where each color corresponds to the category\n",
        "    if len(labels) > 1:\n",
        "        plt.colorbar(scatter, label='Directory')\n",
        "\n",
        "    # Add legend if we have multiple categories\n",
        "    if len(labels) > 1 and len(labels) <= 10:\n",
        "        handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                            markerfacecolor=plt.cm.tab10(i/len(labels)),\n",
        "                            markersize=8, label=label)\n",
        "                  for i, label in enumerate(labels)]\n",
        "        ax.legend(handles=handles, title='Directory', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Dimension 1')\n",
        "    ax.set_ylabel('Dimension 2')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return embeddings_2d\n",
        "\n"
      ],
      "metadata": {
        "id": "EFT-_xrCywCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize embeddings using both PCA and t-SNE\n",
        "if len(features) > 0:\n",
        "    print(\"Creating 2D visualizations of the image embeddings...\")\n",
        "\n",
        "    # Standard PCA visualization\n",
        "    pca_embeddings = visualize_embeddings_2d(features, valid_image_paths, method='pca', n_samples=200)\n",
        "\n",
        "    # t-SNE visualization with filename labels\n",
        "    print(\"\\nCreating t-SNE visualization with filename labels...\")\n",
        "    tsne_embeddings = visualize_embeddings_2d(features, valid_image_paths, method='tsne',\n",
        "                                             n_samples=50, show_labels=True)\n",
        "else:\n",
        "    print(\"No features available for visualization!\")"
      ],
      "metadata": {
        "id": "G1frlyo_0tkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's analyze the distribution of similarities and feature statistics.\n",
        "\n",
        "def analyze_embeddings(features, valid_paths):\n",
        "    \"\"\"Analyze the properties of extracted embeddings.\"\"\"\n",
        "    if len(features) == 0:\n",
        "        print(\"No features available for analysis!\")\n",
        "        return\n",
        "\n",
        "    print(\"=== Embedding Analysis ===\")\n",
        "    print(f\"Number of images: {len(features)}\")\n",
        "    print(f\"Feature dimensionality: {features.shape[1]}\")\n",
        "    print(f\"Feature range: [{features.min():.3f}, {features.max():.3f}]\")\n",
        "    print(f\"Mean feature magnitude: {np.linalg.norm(features, axis=1).mean():.3f}\")\n",
        "\n",
        "    # Compute pairwise similarities\n",
        "    print(\"\\nComputing pairwise similarities...\")\n",
        "    similarities = cosine_similarity(features)\n",
        "\n",
        "    # Remove diagonal (self-similarities)\n",
        "    np.fill_diagonal(similarities, 0)\n",
        "\n",
        "    # Analyze similarity distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Histogram of similarities\n",
        "    axes[0].hist(similarities.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
        "    axes[0].set_title('Distribution of Pairwise Similarities')\n",
        "    axes[0].set_xlabel('Cosine Similarity')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Feature magnitude distribution\n",
        "    feature_magnitudes = np.linalg.norm(features, axis=1)\n",
        "    axes[1].hist(feature_magnitudes, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[1].set_title('Distribution of Feature Magnitudes')\n",
        "    axes[1].set_xlabel('L2 Norm')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Average pairwise similarity: {similarities.mean():.3f}\")\n",
        "    print(f\"Std of pairwise similarities: {similarities.std():.3f}\")\n",
        "    print(f\"Most similar pair similarity: {similarities.max():.3f}\")\n",
        "\n",
        "# Run analysis\n",
        "if len(features) > 0:\n",
        "    analyze_embeddings(features, valid_image_paths)"
      ],
      "metadata": {
        "id": "lcXHi7j4yys4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity ranges from -1 to +1, where 1 = identical, 0 = orthogonal, -1 = opposite\n",
        "+ If average pairwise similarity is high, then the images are sharing similar kinds of composition, colours, visual organization etc.\n",
        "+ The standard deviation (std) is low, then this suggests that there is consistent similarity, limited diversity, tight clustering, or a homogeneous dataset.\n",
        "+ If there is a most similar pair scoring 1.000, then that suggests that there's a duplicate image in the dataset somewhere!\n",
        "\n",
        "So... what does this all imply about our dataset? Make a note!"
      ],
      "metadata": {
        "id": "97DGKlqSy1Ul"
      }
    }
  ]
}