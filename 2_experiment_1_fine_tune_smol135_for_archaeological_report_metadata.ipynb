{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnQVXwQsEWND"
      },
      "source": [
        "The goal is to fine-tune smol17 to return useful metadata for categorizing archaeological reports in Britain.\n",
        "\n",
        "The source data came from the Archaeology Data Service, 1173 report metadata records returned from a simple search of 'Roman'. 'Bibliography' and 'Url' were removed from the columns. Training data is in `ads-roman-result.csv`\n",
        "\n",
        "Guidance from https://mikulskibartosz.name/fine-tune-small-language-model. Code pair-programmed with Gemini 2 and Claude Sonnet models.\n",
        "\n",
        "Look: I never said I was a good programmer. Rather, I start with what I have, where I want to go, I look up tutorials, I adapt as I can, and I use the models to understand the errors and find (limited) solutions that I understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VzdqeggETTc",
        "outputId": "e5e21846-747f-4c23-d8e7-fbcb3b56150c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-21 14:41:07--  https://gist.githubusercontent.com/shawngraham/d71c21640e1597d90c02123c290c9472/raw/372b865372872fdd78bf1f222bd2299b3217863b/ads-roman-result.csv\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1221046 (1.2M) [text/plain]\n",
            "Saving to: \u2018ads-roman-result.csv\u2019\n",
            "\n",
            "ads-roman-result.cs 100%[===================>]   1.16M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-12-21 14:41:07 (16.8 MB/s) - \u2018ads-roman-result.csv\u2019 saved [1221046/1221046]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/shawngraham/d71c21640e1597d90c02123c290c9472/raw/372b865372872fdd78bf1f222bd2299b3217863b/ads-roman-result.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSNXSxB3FBeC"
      },
      "source": [
        "## Turn the data into training data\n",
        "\n",
        "We need to turn the data into examples of what we want smol to eventually do. The following code reads the comma delimited results from ADS (which use semi-colons _within_ columns to indicated lists) into jsonl data with example prompt and response text.\n",
        "\n",
        "(The hardest part of all of this is finding, and formatting, training data).\n",
        "\n",
        "The following code block will parse the csv and map the information in the desired format. It will write `processed_data.json` which is the csv data represented as json, and then `training_data.jsonl` which is in the json lines format, and is what the fine-tuner requires with a system prompt/question and the training data formatted as an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deCfNNU7E_v1",
        "outputId": "20c38444-d467-41d5-92fe-949504c0f40f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-f6765e65d22d>:110: ParserWarning: Skipping line 286: expected 6 fields, saw 23\n",
            "\n",
            "  df = pd.read_csv(input_file,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available columns in CSV:\n",
            "- Title\n",
            "- Description\n",
            "- Location\n",
            "- PeriodSubjectIntervention\n",
            "- Indentifiers\n",
            "- People\n",
            "\n",
            "First row of data:\n",
            "{'Title': 'Images and CAD Data from the Island and PSS Soil Bund Sites phase of Archaeological Mitigation Work at the Houghton Regis North 1 Development, Bedfordshire, 2021', 'Description': 'This collection comprises images and CAD data from archaeological work by Albion Archaeology, in advance of development at Houghton Regis North 1 (HRN1) or Linmere, north of Houghton Regis, Bedfordshire. This particular phase of work was undertaken between 21st June and 23rd August 2021, and comprised archaeological monitoring and a Strip, Map and Sample on the Island and PSS Soil Bund Sites (AIA3C) area of the proposed development site.', 'Location': 'Civil Parish:Chalton;Named Location:Island and PSS Soil Bund Sites;Grid Ref:TL036256;District:Central Bedfordshire;Admin County:Bedfordshire;Country:England;Civil Parish:Houghton Regis;Named Location:Linmere;EPSG:27700:503668;EPSG:27700:225669;', 'PeriodSubjectIntervention': 'Subject:Sherd;Subject:Field Observation (Monitoring);Subject:Field Boundary;Subject:Excavations (Archaeology)--England;Subject:Post Hole;Subject:Quarry;Subject:Archaeology;Subject:Strip Map And Sample;Period:POST MEDIEVAL;Period:-800 - 1800;Period:ROMAN;Subject:Ridge And Furrow;Period:IRON AGE;Subject:Pit;Subject:Ditch;', 'Indentifiers': 'Associated ID:OASIS ID: albionar1-412507;Import RCN:ICDIPSBSAMWHRNDB23-01;Associated ID:DOI: 10.5284/1116551;', 'People': 'Creator:Albion Archaeology;'}\n",
            "\n",
            "Saved raw CSV content to debug_raw.txt for inspection\n",
            "\n",
            "DataFrame shape: (849, 6)\n",
            "\n",
            "First few lines of raw file:\n",
            "Title, Description, Location, PeriodSubjectIntervention, Indentifiers, People\n",
            "\n",
            "\"Images and CAD Data from the Island and PSS Soil Bund Sites phase of Archaeological Mitigation Work at the Houghton Regis North 1 Development, Bedfordshire, 2021\",\"This collection comprises images and CAD data from archaeological work by Albion Archaeology, in advance of development at Houghton Regis North 1 (HRN1) or Linmere, north of Houghton Regis, Bedfordshire. This particular phase of work was undertaken between 21st June and 23rd August 2021, and comprised archaeological monitoring and a Strip, Map and Sample on the Island and PSS Soil Bund Sites (AIA3C) area of the proposed development site.\",Civil Parish:Chalton;Named Location:Island and PSS Soil Bund Sites;Grid Ref:TL036256;District:Central Bedfordshire;Admin County:Bedfordshire;Country:England;Civil Parish:Houghton Regis;Named Location:Linmere;EPSG:27700:503668;EPSG:27700:225669;,Subject:Sherd;Subject:Field Observation (Monitoring);Subject:Field Boundary;Subject:Excavations (Archaeology)--England;Subject:Post Hole;Subject:Quarry;Subject:Archaeology;Subject:Strip Map And Sample;Period:POST MEDIEVAL;Period:-800 - 1800;Period:ROMAN;Subject:Ridge And Furrow;Period:IRON AGE;Subject:Pit;Subject:Ditch;,Associated ID:OASIS ID: albionar1-412507;Import RCN:ICDIPSBSAMWHRNDB23-01;Associated ID:DOI: 10.5284/1116551;,Creator:Albion Archaeology;\n",
            "\n",
            "\n",
            "Example of processed JSON:\n",
            "{\n",
            "  \"title\": \"Images and CAD Data from the Island and PSS Soil Bund Sites phase of Archaeological Mitigation Work at the Houghton Regis North 1 Development, Bedfordshire, 2021\",\n",
            "  \"description\": \"This collection comprises images and CAD data from archaeological work by Albion Archaeology, in advance of development at Houghton Regis North 1 (HRN1) or Linmere, north of Houghton Regis, Bedfordshire. This particular phase of work was undertaken between 21st June and 23rd August 2021, and comprised archaeological monitoring and a Strip, Map and Sample on the Island and PSS Soil Bund Sites (AIA3C) area of the proposed development site.\",\n",
            "  \"location\": {\n",
            "    \"Civil Parish\": \"Houghton Regis\",\n",
            "    \"Named Location\": \"Linmere\",\n",
            "    \"Grid Ref\": \"TL036256\",\n",
            "    \"District\": \"Central Bedfordshire\",\n",
            "    \"Admin County\": \"Bedfordshire\",\n",
            "    \"Country\": \"England\",\n",
            "    \"EPSG\": \"27700:225669\"\n",
            "  },\n",
            "  \"period_subject\": {\n",
            "    \"periods\": [\n",
            "      \"POST MEDIEVAL\",\n",
            "      \"-800 - 1800\",\n",
            "      \"ROMAN\",\n",
            "      \"IRON AGE\"\n",
            "    ],\n",
            "    \"subjects\": [\n",
            "      \"Sherd\",\n",
            "      \"Field Observation (Monitoring)\",\n",
            "      \"Field Boundary\",\n",
            "      \"Excavations (Archaeology)--England\",\n",
            "      \"Post Hole\",\n",
            "      \"Quarry\",\n",
            "      \"Archaeology\",\n",
            "      \"Strip Map And Sample\",\n",
            "      \"Ridge And Furrow\",\n",
            "      \"Pit\",\n",
            "      \"Ditch\"\n",
            "    ]\n",
            "  },\n",
            "  \"identifiers\": {\n",
            "    \"Associated ID\": \"DOI: 10.5284/1116551\",\n",
            "    \"Import RCN\": \"ICDIPSBSAMWHRNDB23-01\"\n",
            "  },\n",
            "  \"people\": [\n",
            "    \"Creator:Albion Archaeology\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Example of training format:\n",
            "<|system|>You are a helpful archaeological assistant trained to identify appropriate metadata from archaeological reports.\n",
            "<|user|>Please identify the metadata that describes the work recounted in this archaeological report from Linmere: Images and CAD Data from the Island and PSS Soil Bund Sites phase of Archaeological Mitigation Work at the Houghton Regis North 1 Development, Bedfordshire, 2021 This collection comprises images and CAD data from archaeological work by Albion Archaeology, in advance of development at Houghton Regis North 1 (HRN1) or Linmere, north of Houghton Regis, Bedfordshire. This particular phase of work was undertaken between 21st June and 23rd August 2021, and comprised archaeological monitoring and a Strip, Map and Sample on the Island and PSS Soil Bund Sites (AIA3C) area of the proposed development site. {'Civil Parish': 'Houghton Regis', 'Named Location': 'Linmere', 'Grid Ref': 'TL036256', 'District': 'Central Bedfordshire', 'Admin County': 'Bedfordshire', 'Country': 'England', 'EPSG': '27700:225669'} {'periods': ['POST MEDIEVAL', '-800 - 1800', 'ROMAN', 'IRON AGE'], 'subjects': ['Sherd', 'Field Observation (Monitoring)', 'Field Boundary', 'Excavations (Archaeology)--England', 'Post Hole', 'Quarry', 'Archaeology', 'Strip Map And Sample', 'Ridge And Furrow', 'Pit', 'Ditch']} ['Creator:Albion Archaeology'] {'Associated ID': 'DOI: 10.5284/1116551', 'Import RCN': 'ICDIPSBSAMWHRNDB23-01'}\n",
            "<|assistant|>{\"location\": {\"civil_parish\": \"Houghton Regis\", \"admin_county\": \"Bedfordshire\", \"subjects\": [\"Sherd\", \"Field Observation (Monitoring)\", \"Field Boundary\", \"Excavations (Archaeology)--England\", \"Post Hole\", \"Quarry\", \"Archaeology\", \"Strip Map And Sample\", \"Ridge And Furrow\", \"Pit\", \"Ditch\"], \"periods\": [\"POST MEDIEVAL\", \"-800 - 1800\", \"ROMAN\", \"IRON AGE\"], \"work_conducted_by\": [\"Creator:Albion Archaeology\"], \"identifiers\": {\"Associated ID\": \"DOI: 10.5284/1116551\", \"Import RCN\": \"ICDIPSBSAMWHRNDB23-01\"}}}<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# for use with ADS csv download file that mixes comma delimited fields with semicolon delimited lists\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "def parse_delimited_field(field, delimiter=';'):\n",
        "    \"\"\"Parse semicolon-delimited fields into lists, cleaning empty entries\"\"\"\n",
        "    if pd.isna(field):\n",
        "        return []\n",
        "    items = [item.strip() for item in str(field).split(delimiter)]\n",
        "    return [item for item in items if item]\n",
        "\n",
        "def parse_location_data(location_str):\n",
        "    \"\"\"Parse location string into structured format\"\"\"\n",
        "    if pd.isna(location_str):\n",
        "        return {}\n",
        "\n",
        "    location_data = {}\n",
        "    parts = location_str.split(';')\n",
        "\n",
        "    for part in parts:\n",
        "        if ':' in part:\n",
        "            key, value = part.split(':', 1)\n",
        "            location_data[key.strip()] = value.strip()\n",
        "        elif 'EPSG' in part:  # Handle EPSG coordinates\n",
        "            coord_type, value = part.split(':', 2)[1:]\n",
        "            location_data[f'EPSG_{coord_type}'] = value\n",
        "\n",
        "    return location_data\n",
        "\n",
        "def parse_period_subject(field):\n",
        "    \"\"\"Parse period and subject information into categorized lists\"\"\"\n",
        "    if pd.isna(field):\n",
        "        return {'periods': [], 'subjects': []}\n",
        "\n",
        "    periods = []\n",
        "    subjects = []\n",
        "\n",
        "    items = parse_delimited_field(field)\n",
        "    for item in items:\n",
        "        if item.startswith('Period:'):\n",
        "            periods.append(item.replace('Period:', '').strip())\n",
        "        elif item.startswith('Subject:'):\n",
        "            subjects.append(item.replace('Subject:', '').strip())\n",
        "\n",
        "    return {\n",
        "        'periods': periods,\n",
        "        'subjects': subjects\n",
        "    }\n",
        "\n",
        "def parse_identifiers(identifier_str):\n",
        "    \"\"\"Parse identifier string into structured format\"\"\"\n",
        "    if pd.isna(identifier_str):\n",
        "        return {}\n",
        "\n",
        "    identifiers = {}\n",
        "    parts = parse_delimited_field(identifier_str)\n",
        "\n",
        "    for part in parts:\n",
        "        if ':' in part:\n",
        "            key, value = part.split(':', 1)\n",
        "            identifiers[key.strip()] = value.strip()\n",
        "\n",
        "    return identifiers\n",
        "\n",
        "def transform_row_to_json(row):\n",
        "    \"\"\"Transform a single row into structured JSON format\"\"\"\n",
        "    return {\n",
        "        'title': row['Title'],\n",
        "        'description': row['Description'],\n",
        "        'location': parse_location_data(row['Location']),\n",
        "        'period_subject': parse_period_subject(row['PeriodSubjectIntervention']),\n",
        "        'identifiers': parse_identifiers(row['Indentifiers']),\n",
        "        'people': parse_delimited_field(row['People'])\n",
        "    }\n",
        "\n",
        "# this is for creating a dataset for finetuning smol\n",
        "def format_for_training(entry):\n",
        "    \"\"\"Format the JSON entry into training format\"\"\"\n",
        "    # Create instruction from available data\n",
        "    instruction = (\n",
        "        f\"Please identify the metadata that describes the work recounted in this archaeological report from {entry['location'].get('Named Location', 'unknown location')}: \"\n",
        "        f\"{entry['title']} {entry['description']} {entry['location']} {entry['period_subject']} {entry['people']} {entry['identifiers']}\"\n",
        "    )\n",
        "\n",
        "    # Create response using structured data\n",
        "    response = {\n",
        "        \"location\": {\n",
        "            \"civil_parish\": entry['location'].get('Civil Parish', ''),\n",
        "             \"admin_county\": entry['location'].get('Admin County', ''),\n",
        "        \"subjects\": entry['period_subject']['subjects'],\n",
        "        \"periods\": entry['period_subject']['periods'],\n",
        "        \"work_conducted_by\": entry['people'],\n",
        "        \"identifiers\": entry['identifiers']\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"<|system|>You are a helpful archaeological assistant trained to identify appropriate metadata from archaeological reports.\\n\"\n",
        "                f\"<|user|>{instruction}\\n\"\n",
        "                f\"<|assistant|>{json.dumps(response)}<|endoftext|>\"\n",
        "    }\n",
        "\n",
        "def process_archaeological_csv(input_file, output_json=\"processed_data.json\", output_training=\"training_data.jsonl\"):\n",
        "    \"\"\"Process archaeological CSV file into JSON and training format\"\"\"\n",
        "    try:\n",
        "        # Read CSV using pandas, handle quote char issues\n",
        "        try:\n",
        "            df = pd.read_csv(input_file,\n",
        "                               quotechar='\"',\n",
        "                               escapechar='\\\\',\n",
        "                               encoding='utf-8',\n",
        "                               on_bad_lines='warn')\n",
        "        except Exception as e:\n",
        "             print(f\"Error during initial read with quotes:\\n{e}\\n trying without quotes\")\n",
        "             try:\n",
        "                  df = pd.read_csv(input_file,\n",
        "                                   encoding='utf-8',\n",
        "                                   on_bad_lines='warn')\n",
        "             except Exception as e:\n",
        "                  print(f\"Error during initial read without quotes:\\n{e}\")\n",
        "                  raise e\n",
        "\n",
        "        # Remove leading/trailing spaces from column names\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # Debug: Print column names and first row\n",
        "        print(\"\\nAvailable columns in CSV:\")\n",
        "        for col in df.columns:\n",
        "            print(f\"- {col}\")\n",
        "\n",
        "        print(\"\\nFirst row of data:\")\n",
        "        print(df.iloc[0].to_dict())\n",
        "\n",
        "        # Save raw CSV content for debugging\n",
        "        with open('debug_raw.txt', 'w', encoding='utf-8') as f:\n",
        "            with open(input_file, 'r', encoding='utf-8') as src:\n",
        "                f.write(src.read())\n",
        "\n",
        "        print(\"\\nSaved raw CSV content to debug_raw.txt for inspection\")\n",
        "\n",
        "        # Print shape of dataframe\n",
        "        print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "\n",
        "        # Print first few lines of raw file\n",
        "        print(\"\\nFirst few lines of raw file:\")\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            print(f.readline())  # Header\n",
        "            print(f.readline())  # First data row\n",
        "\n",
        "        processed_data = df.apply(transform_row_to_json, axis=1).tolist()\n",
        "\n",
        "        training_data = [format_for_training(entry) for entry in processed_data]\n",
        "\n",
        "        # Output processed data as json\n",
        "        with open(output_json, \"w\") as f:\n",
        "            json.dump(processed_data, f, indent=2)\n",
        "\n",
        "        # Output training data as jsonl\n",
        "        with open(output_training, \"w\") as f:\n",
        "            for entry in training_data:\n",
        "                json.dump(entry, f)\n",
        "                f.write('\\n')\n",
        "\n",
        "        return processed_data, training_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during processing:\")\n",
        "        print(f\"Type of error: {type(e)}\")\n",
        "        print(f\"Error message: {str(e)}\")\n",
        "        if 'df' in locals():\n",
        "            print(\"\\nDataFrame Info:\")\n",
        "            print(df.info())\n",
        "        raise e\n",
        "\n",
        "\n",
        "# run the code\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        processed_data, training_data = process_archaeological_csv('ads-roman-result.csv')\n",
        "\n",
        "        # Print example of processed data\n",
        "        print(\"\\nExample of processed JSON:\")\n",
        "        print(json.dumps(processed_data[0], indent=2))\n",
        "\n",
        "        print(\"\\nExample of training format:\")\n",
        "        print(training_data[0]['text'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWk-MYx9ZXzN"
      },
      "source": [
        "Now we need to get the Smol-135 model. The code below will download it from Huggingface, a site that functions as a repository for models and data. You need to make an account there, and get an access token for the api. Once you have your token, click on the 'key' icon ('secrets') at the left of the screen, and create a new secret called `HF_TOKEN`. Paste your token in. Now you can access these models *and* push your fine-tuned version to your own user account for use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My0Lij2tT7hC"
      },
      "outputs": [],
      "source": [
        "## now we get set up with a model\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "eos_string = tokenizer.decode([tokenizer.eos_token_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3g1-2iyZ39R"
      },
      "source": [
        "The next block installs some more packages that we will need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME7VTCEgXyGF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz0rX5P-Z8vB"
      },
      "source": [
        "This block creates our fine tuning, taking our dataset and splitting it into training and testing splits (so we can get a sense of how well the model is trained). The arguments for _how_ to train the model can be adjusted and explored. This block also sets the trainer running. I find it helpful sometimes to copy the output (the training quality statistics) and the training arguments into a model like Claude.ai Sonnet and ask the model to 'interpret these results then suggest more effective settings', just to see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463,
          "referenced_widgets": [
            "062481d32726453cbfeb4a068fbe0df9",
            "098ad43d22de4202bb8e4018be592096",
            "71f0c779f075446087ab856f8c794776",
            "b75370bb24db462f9bbeb90bd6e56b13",
            "b4e4fbc2f16d4fd38d138e3f699fdb62",
            "a0bad704dc3f44f39664b6d1ad152bc1",
            "19715bd19c5b4296b51da3a41dae1592",
            "503b63b38910470a9054b2c5d7dd3c77",
            "e6a58b4bfaf546679d8cfe2b0a1006d5",
            "45b1df07888c4069a3fe9b03820c73b9",
            "e8def8cd2a2f44d6bbe9b8e3a99aa0d9"
          ]
        },
        "id": "GEEGgprBVSMt",
        "outputId": "0b86a71d-0b6b-4788-e3c6-61477145f57b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first row of dataset before mapping: {'text': '<|system|>You are a helpful archaeological assistant trained to categorize archaeological reports.\\n<|user|>Please categorize this archaeological report metadata from Linmere: This collection comprises images and CAD data from archaeological work by Albion Archaeology, in advance of development at Houghton Regis North 1 (HRN1) or Linmere, north of Houghton Regis, Bedfordshire. This particular phase of work was undertaken between 21st June and 23rd August 2021, and comprised archaeological monitoring and a Strip, Map and Sample on the Island and PSS Soil Bund Sites (AIA3C) area of the proposed development site.\\n<|assistant|>{\"subjects\": [\"Sherd\", \"Field Observation (Monitoring)\", \"Field Boundary\", \"Excavations (Archaeology)--England\", \"Post Hole\", \"Quarry\", \"Archaeology\", \"Strip Map And Sample\", \"Ridge And Furrow\", \"Pit\", \"Ditch\"], \"periods\": [\"POST MEDIEVAL\", \"-800 - 1800\", \"ROMAN\", \"IRON AGE\"], \"work_conducted_by\": [\"Creator:Albion Archaeology\"], \"location\": {\"civil_parish\": \"Houghton Regis\", \"admin_county\": \"Bedfordshire\"}}<|endoftext|>'}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "062481d32726453cbfeb4a068fbe0df9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/849 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first row of dataset after mapping: {'input_ids': [44, 108, 9690, 108, 46, 2683, 359, 253, 5356, 13753, 11173, 7018, 288, 31239, 13753, 4631, 30, 198, 44, 108, 4093, 108, 46, 10180, 31239, 451, 13753, 1378, 11566, 429, 9565, 42596, 42, 669, 3854, 15658, 3265, 284, 24218, 940, 429, 13753, 746, 411, 19726, 285, 26287, 28, 281, 4408, 282, 1421, 418, 42674, 3192, 271, 2601, 216, 33, 365, 15416, 62, 33, 25, 355, 9565, 42596, 28, 4081, 282, 42674, 3192, 271, 28, 37201, 21225, 30, 669, 1542, 5239, 282, 746, 436, 14999, 826, 216, 34, 33, 302, 4019, 284, 216, 34, 35, 7212, 4053, 216, 34, 32, 34, 33, 28, 284, 15556, 13753, 5293, 284, 253, 41529, 28, 11563, 284, 21530, 335, 260, 5378, 284, 377, 6690, 15960, 27969, 25730, 365, 49, 7854, 35, 51, 25, 1557, 282, 260, 5433, 1421, 2530, 30, 198, 44, 108, 520, 9531, 108, 46, 39428, 19541, 99, 1799, 9523, 47415, 84, 1002, 476, 5468, 35300, 365, 41155, 23849, 476, 5468, 44842, 1002, 476, 32223, 565, 491, 365, 37318, 899, 25, 423, 47063, 1002, 476, 10360, 32381, 1002, 476, 6317, 15875, 1002, 476, 37318, 899, 1002, 476, 1393, 6696, 11563, 1350, 21530, 1002, 476, 66, 3177, 1350, 33817, 720, 1002, 476, 64, 269, 1002, 476, 52, 3224, 9224, 476, 17439, 99, 1799, 9523, 17570, 32983, 57, 16091, 3206, 1002, 18395, 40, 32, 32, 731, 216, 33, 40, 32, 32, 1002, 476, 12988, 2810, 1002, 476, 5810, 2154, 330, 10992, 9224, 476, 1637, 79, 20445, 277, 79, 1717, 1799, 9523, 24727, 270, 42, 2219, 82, 285, 26287, 9224, 476, 10786, 1799, 9583, 35751, 79, 1095, 621, 1799, 476, 56, 35947, 3192, 271, 1002, 476, 14299, 79, 4639, 105, 1799, 476, 44912, 4157, 21225, 18, 17859, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-92d9f06469f9>:89: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset Length 806\n",
            "Validation Dataset Length 43\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 04:02, Epoch 7/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.317705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>44.372200</td>\n",
              "      <td>3.066287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>44.372200</td>\n",
              "      <td>2.665973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>33.430600</td>\n",
              "      <td>2.267325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>33.430600</td>\n",
              "      <td>1.931662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>27.542900</td>\n",
              "      <td>1.750549</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=30, training_loss=35.11522216796875, metrics={'train_runtime': 251.3707, 'train_samples_per_second': 32.064, 'train_steps_per_second': 0.119, 'total_flos': 1176436922803200.0, 'train_loss': 35.11522216796875, 'epoch': 7.627450980392156})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"HuggingFaceTB/SmolLM-135M\"\n",
        "\n",
        "# Load base model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def load_and_preprocess_data(data_path, data_type='jsonl'):\n",
        "    \"\"\"\n",
        "    Load archaeological reports and preprocess them for training\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    if data_type == 'jsonl':\n",
        "        if not os.path.exists(data_path):\n",
        "           raise FileNotFoundError(f\"Error: The file '{data_path}' could not be found.\")\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "           for line in f:\n",
        "              try:\n",
        "                  entry = json.loads(line)\n",
        "                  texts.append(entry['text'])\n",
        "              except json.JSONDecodeError:\n",
        "                 print(f\"Warning: Could not decode line as JSON: {line.strip()}\")\n",
        "    elif data_type == 'csv':\n",
        "        if not os.path.exists(data_path):\n",
        "           raise FileNotFoundError(f\"Error: The file '{data_path}' could not be found.\")\n",
        "        df = pd.read_csv(data_path)\n",
        "        print(f\"Shape of df: {df.shape}\") # check shape of dataframe\n",
        "        print(f\"first 5 rows of df: {df.head()}\") # check dataframe content\n",
        "        texts = df['text'].tolist()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid data_type. Must be 'jsonl' or 'csv'.\")\n",
        "\n",
        "    return Dataset.from_pandas(pd.DataFrame({'text': texts}))\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=300, return_tensors=\"pt\") # modified max_length\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data_path=\"training_data.jsonl\"\n",
        "data_type=\"jsonl\"\n",
        "dataset = load_and_preprocess_data(data_path, data_type)\n",
        "print(f\"first row of dataset before mapping: {dataset[0]}\") # check content before mapping\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "print(f\"first row of dataset after mapping: {tokenized_dataset[0]}\") # check content after mapping\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.05)\n",
        "\n",
        "\n",
        "# Create Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "\n",
        "# Training Arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"SmolLM\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,       # Increased batch size if memory allows\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    logging_dir=\"./logs\",                 # Add logging\n",
        "    logging_steps=50,\n",
        "    fp16=True,                           # Enable mixed precision training if available\n",
        "    gradient_accumulation_steps=2,        # Accumulate gradients for larger effective batch size\n",
        "    warmup_steps=500,                    # Add warmup steps\n",
        "    seed=42,                             # Set random seed for reproducibility\n",
        "    report_to=\"none\",                    # without this, colab logs the run with 'weights and biases' service, which requires an api etc\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        ")\n",
        "print(f\"Training Dataset Length {len(tokenized_dataset['train'])}\") # checking training data length\n",
        "print(f\"Validation Dataset Length {len(tokenized_dataset['test'])}\") # checking test data length\n",
        "\n",
        "# Do the Training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-0Jc07eakvb"
      },
      "source": [
        "Test! In this next block, I can paste in the archaeological site description after where it says `...return json:`\n",
        "\n",
        "```\n",
        "prompt = \"<|system|>You are a helpful archaeological assistant trained to\n",
        "categorize archaeological reports.\\n<|user|>Please categorize this\n",
        "archaeological report metadata; return json: This collection comprises images\n",
        "and CAD data from archaeological work by GrahamCo Archaeology, in advance of\n",
        "development at Claygate North 1 (CLYGT1). This particular phase of work was\n",
        "undertaken between 21st June and 23rd August 2021, and comprised archaeological\n",
        "monitoring and a Strip, Map and Sample on the area of the proposed development\n",
        "site. Roman period Samian ware from the 1st and second centuries were\n",
        "recovered. \\n\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjPGbJAHwf2f"
      },
      "outputs": [],
      "source": [
        "# test out before saving\n",
        "trained_model = trainer.model\n",
        "\n",
        "prompt = \"<|system|>You are a helpful archaeological assistant trained to categorize archaeological reports.\\n<|user|>Please categorize this archaeological report metadata; return json: This collection comprises images and CAD data from archaeological work by GrahamCo Archaeology, in advance of development at Claygate North 1 (CLYGT1). This particular phase of work was undertaken between 21st June and 23rd August 2021, and comprised archaeological monitoring and a Strip, Map and Sample on the area of the proposed development site. Roman period Samian ware from the 1st and second centuries were recovered. \\n\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "\n",
        "generated_ids = trained_model.generate(\n",
        " input_ids,\n",
        "    max_new_tokens=300,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttfxrJWabLHi"
      },
      "source": [
        "If you want to try retraining, you should run this next block then go back to the training block and rerun. But if you want to save your model, DO NOT run this block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIn60tZ-wbb-"
      },
      "outputs": [],
      "source": [
        "#clean up and free memory\n",
        "# remove # to run this:\n",
        "\n",
        "#del model\n",
        "#del tokenizer\n",
        "#del trainer\n",
        "#del tokenized_dataset\n",
        "#del dataset\n",
        "#gc.collect()\n",
        "#torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv4WjLP_a_3v"
      },
      "source": [
        "This code block will push your model to your account on huggingface. Change where it says `your-username`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpWE2_ITfx5w"
      },
      "outputs": [],
      "source": [
        "\n",
        "#save to hugginface.\n",
        "#it will ask for your token and then like github push the model\n",
        "#to your space, ie huggingface.com/your-username/name-of-your-new-model-as-specified-below\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# First, login to Hugging Face\n",
        "notebook_login()\n",
        "\n",
        "# Save the model and all necessary files\n",
        "trainer.save_model(\"./SmolLM\")\n",
        "\n",
        "# Save the tokenizer and label map with the model\n",
        "tokenizer.save_pretrained(\"./SmolLM\")\n",
        "\n",
        "# Push to hub with a model card\n",
        "model.push_to_hub(\"your-username/Smol_archae_metadata_model\",\n",
        "    use_auth_token=True,\n",
        "    model_card_kwargs={\n",
        "        \"language\": \"en\",\n",
        "        \"license\": \"mit\",\n",
        "        \"datasets\": [\"custom archaeology dataset\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Push the tokenizer configuration\n",
        "tokenizer.push_to_hub(\"your-username/Smol_archae_metadata_model\")\n",
        "\n",
        "# Initialize the Hugging Face API\n",
        "api = HfApi()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPG4RevQbgpG"
      },
      "source": [
        "Here's some code to load your model from Huggingface, ie, if you were coming back to the computer after a hiatus and you didn't want to retrain everything from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvLmAqhtgJMA"
      },
      "outputs": [],
      "source": [
        "# Smol135 versus your fine tuned model\n",
        "\n",
        "# and here we're going to load the models from huggingface and test them against the same output\n",
        "\n",
        "# so you can see the difference between fine-tuning the original smol-135 makes\n",
        "# of course, the result might not yet be much good, but if you paid for more memory/time on the gpu it'd get better\n",
        "# and also, if you did more with the training data - and more data - to make sure it was how you wanted it...\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import json\n",
        "\n",
        "def generate_response_pipeline(pipeline, input_text, max_length=500, temperature=0):\n",
        "    \"\"\"\n",
        "    Generates a response using a Hugging Face pipeline\n",
        "    \"\"\"\n",
        "    response = pipeline(input_text, max_length=max_length, temperature=0)[0]['generated_text']\n",
        "\n",
        "    return response\n",
        "\n",
        "def generate_response(model, tokenizer, input_text, max_length=500, temperature=0):\n",
        "    \"\"\"\n",
        "    Generates a response using the model\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=max_length)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def load_and_prepare_model(model_path, model_id=None, use_merged = True):\n",
        "    \"\"\"\n",
        "    Loads and prepares either the merged model or the LoRA model\n",
        "    \"\"\"\n",
        "    if use_merged:\n",
        "         # Load the merged model directly\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "    else:\n",
        "        # Load base model and LoRA weights\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "             device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(model, model_path)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path) if use_merged else AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Model names\n",
        "    base_model_id = \"HuggingFaceTB/SmolLM-135M\"\n",
        "    fine_tuned_model_id = \"sgraham/merged_archae_metadata_model\"  # Replace with your actual model repo\n",
        "\n",
        "     # Load the base model\n",
        "    base_model, base_tokenizer = load_and_prepare_model(model_path=base_model_id, model_id=base_model_id, use_merged = True)\n",
        "\n",
        "   # Load the fine-tuned model\n",
        "    ft_model, ft_tokenizer = load_and_prepare_model(model_path=fine_tuned_model_id, model_id=base_model_id, use_merged = True)\n",
        "\n",
        "\n",
        "    # Load the pipeline\n",
        "    ft_pipeline = pipeline(\"text-generation\", model=fine_tuned_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "\n",
        "    # Test example (ensure you format as used in fine-tuning)\n",
        "    prompt = \"\"\"<|system|>You are a helpful archaeological assistant trained to identify appropriate metadata from archaeological reports.\\n<|user|>Please identify the metadata that describes the work recounted in this archaeological report; return json: This collection comprises images and CAD from an archaeological evaluation and watching brief, undertaken by Cotswold Archaeology in August 2018, at Hewmar House, 120 London Road, Gloucester, Gloucestershire. Four archaeological evaluation trenches were excavated and four geotechnical test pits were  observed. Despite the proximity of the site to Wotton Roman cemetery, no evidence for any in situ burials, or indeed any Roman activity, was identified in any of the excavated trenches or test pits. It is likely that the site lay beyond the southern boundary of the cemetery and formed  part of the agricultural hinterland of both Roman and medieval Gloucester until the  construction of Hillfield Villa (later Hewmar House) in the early 19th century. Three linear  garden features, probably planting trenches, associated with Hillfield Villa and a large undated ditch were identified. Evidence for possible quarrying was also identified throughout the site. Periods: POST MEDIEVAL, 1800 - 1850, UNCERTAIN. Subjects: Archaeology, Evaluation, DITCH, GARDEN FEATURE, TRIAL TRENCH.\\n\"\"\"\n",
        "\n",
        "    # Generate and compare\n",
        "    print(f\"Input Prompt: {prompt}\")\n",
        "\n",
        "    base_response = generate_response(base_model, base_tokenizer, prompt)\n",
        "    print(f\"\\nBase Model Response:\\n{base_response}\")\n",
        "\n",
        "    #ft_response_direct = generate_response(ft_model, ft_tokenizer, prompt)\n",
        "    #print(f\"\\nFine-tuned Model Response (Direct):\\n{ft_response_direct}\")\n",
        "\n",
        "    ft_response_pipeline = generate_response_pipeline(ft_pipeline, prompt)\n",
        "    print(f\"\\nFine-tuned Model Response (Pipeline):\\n{ft_response_pipeline}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a_VPOtOb356"
      },
      "source": [
        "This last bit is a pipeline for passing rows from a csv to your newly fine-tuned model for archaeological metadata extraction. Data via ADS again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JShUWP6DFlkr"
      },
      "outputs": [],
      "source": [
        "\n",
        "## get some test data\n",
        "# csv where all of the fields have been smooshed into a single column\n",
        "!wget https://gist.githubusercontent.com/shawngraham/15c7cf3e2982d645b0c03c745f12e6bf/raw/b06b6333aa14dd7d40bb14aac79b3434db3afdd0/test.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTirTR2CFtTR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import HfApi\n",
        "import csv\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "def generate_response_pipeline(pipeline, input_text, max_length=500):\n",
        "    \"\"\"\n",
        "    Generates a response using a Hugging Face pipeline\n",
        "    \"\"\"\n",
        "    #response = pipeline(input_text, max_length=max_length)[0]['generated_text']\n",
        "    response = pipeline(input_text, max_length=max_length, temperature=0.0)[0]['generated_text']\n",
        "    return response\n",
        "\n",
        "def generate_response(model, tokenizer, input_text, max_length=500):\n",
        "    \"\"\"\n",
        "    Generates a response using the model\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #outputs = model.generate(**inputs, max_length=max_length)\n",
        "        outputs = model.generate(**inputs, max_length=max_length, temperature=0.0)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def load_and_prepare_model(model_path, model_id=None, use_merged = True):\n",
        "    \"\"\"\n",
        "    Loads and prepares either the merged model or the LoRA model\n",
        "    \"\"\"\n",
        "    if use_merged:\n",
        "         # Load the merged model directly\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "    else:\n",
        "        # Load base model and LoRA weights\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "             device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(model, model_path)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path) if use_merged else AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def main():\n",
        "   # Model names\n",
        "    base_model_id = \"HuggingFaceTB/SmolLM-135M\"\n",
        "    fine_tuned_model_id = \"sgraham/merged_archae_metadata_model\"  # Replace with your actual model repo\n",
        "\n",
        "     # Load the base model\n",
        "    base_model, base_tokenizer = load_and_prepare_model(model_path=base_model_id, model_id=base_model_id, use_merged = True)\n",
        "\n",
        "   # Load the fine-tuned model\n",
        "    ft_model, ft_tokenizer = load_and_prepare_model(model_path=fine_tuned_model_id, model_id=base_model_id, use_merged = True)\n",
        "\n",
        "\n",
        "    # Load the pipeline\n",
        "    ft_pipeline = pipeline(\"text-generation\", model=fine_tuned_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    # Iterate over the rows of the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        prompt = f\"\"\"<|system|>You are a helpful archaeological assistant trained to identify appropriate metadata from archaeological reports.\\n<|user|>Please identify the metadata that describes the work recounted in this archaeological report; return json: {row['description']}\\n\"\"\"\n",
        "\n",
        "        # Generate and compare\n",
        "        print(f\"Input Prompt: {prompt}\")\n",
        "\n",
        "        #base_response = generate_response(base_model, base_tokenizer, prompt)\n",
        "        #print(f\"\\nBase Model Response:\\n{base_response}\")\n",
        "\n",
        "        #ft_response_direct = generate_response(ft_model, ft_tokenizer, prompt)\n",
        "        #print(f\"\\nFine-tuned Model Response (Direct):\\n{ft_response_direct}\")\n",
        "\n",
        "        ft_response_pipeline = generate_response_pipeline(ft_pipeline, prompt)\n",
        "        print(f\"\\nFine-tuned Model Response (Pipeline):\\n{ft_response_pipeline}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}