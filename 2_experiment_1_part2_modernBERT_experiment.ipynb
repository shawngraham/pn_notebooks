{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upmxx-UUcsro"
      },
      "source": [
        "The goal is to fine-tune ModernBERT (https://huggingface.co/blog/modernbert) to return useful metadata for categorizing archaeological reports in Britain.\n",
        "\n",
        "The source data came from the Archaeology Data Service, 1173 report metadata records returned from a simple search of 'Roman'. 'Bibliography' and 'Url' were removed from the columns. Training data is in ads-roman-result.csv\n",
        "\n",
        "You need a huggingface account and access token. If you set that up earlier, it will still be available here - click the key icon to check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPZI7HUCc0NT"
      },
      "source": [
        "Get the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIlCEFZQg-VW",
        "outputId": "ceed5dd8-4978-4436-cec2-d8a3a9699562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-20 18:14:41--  https://gist.githubusercontent.com/shawngraham/d71c21640e1597d90c02123c290c9472/raw/372b865372872fdd78bf1f222bd2299b3217863b/ads-roman-result.csv\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1221046 (1.2M) [text/plain]\n",
            "Saving to: \u2018ads-roman-result.csv\u2019\n",
            "\n",
            "ads-roman-result.cs 100%[===================>]   1.16M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2024-12-20 18:14:43 (157 MB/s) - \u2018ads-roman-result.csv\u2019 saved [1221046/1221046]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/shawngraham/d71c21640e1597d90c02123c290c9472/raw/372b865372872fdd78bf1f222bd2299b3217863b/ads-roman-result.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAIKH0C0c1yw"
      },
      "source": [
        "Now we use the same preprocessing code to convert the csv into the json data we need. We don't use the jsonl data this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brD0IVt1hGz4"
      },
      "outputs": [],
      "source": [
        "# for use with ADS csv download file that mixes comma delimited fields with semicolon delimited lists\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "def parse_delimited_field(field, delimiter=';'):\n",
        "    \"\"\"Parse semicolon-delimited fields into lists, cleaning empty entries\"\"\"\n",
        "    if pd.isna(field):\n",
        "        return []\n",
        "    items = [item.strip() for item in str(field).split(delimiter)]\n",
        "    return [item for item in items if item]\n",
        "\n",
        "def parse_location_data(location_str):\n",
        "    \"\"\"Parse location string into structured format\"\"\"\n",
        "    if pd.isna(location_str):\n",
        "        return {}\n",
        "\n",
        "    location_data = {}\n",
        "    parts = location_str.split(';')\n",
        "\n",
        "    for part in parts:\n",
        "        if ':' in part:\n",
        "            key, value = part.split(':', 1)\n",
        "            location_data[key.strip()] = value.strip()\n",
        "        elif 'EPSG' in part:  # Handle EPSG coordinates\n",
        "            coord_type, value = part.split(':', 2)[1:]\n",
        "            location_data[f'EPSG_{coord_type}'] = value\n",
        "\n",
        "    return location_data\n",
        "\n",
        "def parse_period_subject(field):\n",
        "    \"\"\"Parse period and subject information into categorized lists\"\"\"\n",
        "    if pd.isna(field):\n",
        "        return {'periods': [], 'subjects': []}\n",
        "\n",
        "    periods = []\n",
        "    subjects = []\n",
        "\n",
        "    items = parse_delimited_field(field)\n",
        "    for item in items:\n",
        "        if item.startswith('Period:'):\n",
        "            periods.append(item.replace('Period:', '').strip())\n",
        "        elif item.startswith('Subject:'):\n",
        "            subjects.append(item.replace('Subject:', '').strip())\n",
        "\n",
        "    return {\n",
        "        'periods': periods,\n",
        "        'subjects': subjects\n",
        "    }\n",
        "\n",
        "def parse_identifiers(identifier_str):\n",
        "    \"\"\"Parse identifier string into structured format\"\"\"\n",
        "    if pd.isna(identifier_str):\n",
        "        return {}\n",
        "\n",
        "    identifiers = {}\n",
        "    parts = parse_delimited_field(identifier_str)\n",
        "\n",
        "    for part in parts:\n",
        "        if ':' in part:\n",
        "            key, value = part.split(':', 1)\n",
        "            identifiers[key.strip()] = value.strip()\n",
        "\n",
        "    return identifiers\n",
        "\n",
        "def transform_row_to_json(row):\n",
        "    \"\"\"Transform a single row into structured JSON format\"\"\"\n",
        "    return {\n",
        "        'title': row['Title'],\n",
        "        'description': row['Description'],\n",
        "        'location': parse_location_data(row['Location']),\n",
        "        'period_subject': parse_period_subject(row['PeriodSubjectIntervention']),\n",
        "        'identifiers': parse_identifiers(row['Indentifiers']),\n",
        "        'people': parse_delimited_field(row['People'])\n",
        "    }\n",
        "\n",
        "# this is for creating a dataset for finetuning smol, which is a different experiment\n",
        "def format_for_training(entry):\n",
        "    \"\"\"Format the JSON entry into training format\"\"\"\n",
        "    # Create instruction from available data\n",
        "    instruction = (\n",
        "        f\"Please categorize this archaeological report metadata from {entry['location'].get('Named Location', 'unknown location')}: \"\n",
        "        f\"{entry['description']}\"\n",
        "    )\n",
        "\n",
        "    # Create response using structured data\n",
        "    response = {\n",
        "        \"subjects\": entry['period_subject']['subjects'],\n",
        "        \"periods\": entry['period_subject']['periods'],\n",
        "        \"work_conducted_by\": entry['people'],\n",
        "        \"location\": {\n",
        "            \"civil_parish\": entry['location'].get('Civil Parish', ''),\n",
        "             \"admin_county\": entry['location'].get('Admin County', '')\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"<|system|>You are a helpful archaeological assistant trained to categorize archaeological reports.\\n\"\n",
        "                f\"<|user|>{instruction}\\n\"\n",
        "                f\"<|assistant|>{json.dumps(response)}<|endoftext|>\"\n",
        "    }\n",
        "\n",
        "def process_archaeological_csv(input_file, output_json=\"processed_data.json\", output_training=\"training_data.jsonl\"):\n",
        "    \"\"\"Process archaeological CSV file into JSON and training format\"\"\"\n",
        "    try:\n",
        "        # Read CSV using pandas, handle quote char issues\n",
        "        try:\n",
        "            df = pd.read_csv(input_file,\n",
        "                               quotechar='\"',\n",
        "                               escapechar='\\\\',\n",
        "                               encoding='utf-8',\n",
        "                               on_bad_lines='warn')\n",
        "        except Exception as e:\n",
        "             print(f\"Error during initial read with quotes:\\n{e}\\n trying without quotes\")\n",
        "             try:\n",
        "                  df = pd.read_csv(input_file,\n",
        "                                   encoding='utf-8',\n",
        "                                   on_bad_lines='warn')\n",
        "             except Exception as e:\n",
        "                  print(f\"Error during initial read without quotes:\\n{e}\")\n",
        "                  raise e\n",
        "\n",
        "        # Remove leading/trailing spaces from column names\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # Debug: Print column names and first row\n",
        "        print(\"\\nAvailable columns in CSV:\")\n",
        "        for col in df.columns:\n",
        "            print(f\"- {col}\")\n",
        "\n",
        "        print(\"\\nFirst row of data:\")\n",
        "        print(df.iloc[0].to_dict())\n",
        "\n",
        "        # Save raw CSV content for debugging\n",
        "        with open('debug_raw.txt', 'w', encoding='utf-8') as f:\n",
        "            with open(input_file, 'r', encoding='utf-8') as src:\n",
        "                f.write(src.read())\n",
        "\n",
        "        print(\"\\nSaved raw CSV content to debug_raw.txt for inspection\")\n",
        "\n",
        "        # Print shape of dataframe\n",
        "        print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "\n",
        "        # Print first few lines of raw file\n",
        "        print(\"\\nFirst few lines of raw file:\")\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            print(f.readline())  # Header\n",
        "            print(f.readline())  # First data row\n",
        "\n",
        "        processed_data = df.apply(transform_row_to_json, axis=1).tolist()\n",
        "\n",
        "        training_data = [format_for_training(entry) for entry in processed_data]\n",
        "\n",
        "        # Output processed data as json\n",
        "        with open(output_json, \"w\") as f:\n",
        "            json.dump(processed_data, f, indent=2)\n",
        "\n",
        "        # Output training data as jsonl\n",
        "        with open(output_training, \"w\") as f:\n",
        "            for entry in training_data:\n",
        "                json.dump(entry, f)\n",
        "                f.write('\\n')\n",
        "\n",
        "        return processed_data, training_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during processing:\")\n",
        "        print(f\"Type of error: {type(e)}\")\n",
        "        print(f\"Error message: {str(e)}\")\n",
        "        if 'df' in locals():\n",
        "            print(\"\\nDataFrame Info:\")\n",
        "            print(df.info())\n",
        "        raise e\n",
        "\n",
        "\n",
        "#run it\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        processed_data, training_data = process_archaeological_csv('ads-roman-result.csv')\n",
        "\n",
        "        # Print example of processed data\n",
        "        print(\"\\nExample of processed JSON:\")\n",
        "        print(json.dumps(processed_data[0], indent=2))\n",
        "\n",
        "        print(\"\\nExample of training format:\")\n",
        "        print(training_data[0]['text'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQsXft6dBhI"
      },
      "source": [
        "The next block installs some more packages that we will need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWDNqAsQly33"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8afmVC5dX1q"
      },
      "source": [
        "The next block creates training examples by extracting entities from the structured data we made in the last step. It looks for several types of entities:\n",
        "\n",
        "- Organizations (ORG) from \"Creator\" fields\n",
        "- Locations (LOC) from location information\n",
        "- Periods (PER) from time period data\n",
        "- Subjects (SUBJ) from subject classifications\n",
        "- Identifiers (ID) from identifier fields\n",
        "\n",
        "It then formats these examples using either BIO or BILOU tagging schemes, which are standard approaches for NER tasks:\n",
        "\n",
        "- BIO: Tags tokens as Beginning, Inside, or Outside of an entity\n",
        "- BILOU: More detailed scheme that marks Beginning, Inside, Last, Outside, or Unit-length entities\n",
        "\n",
        "This information is crucial for correctly finding what we're after, when we use the fine-tuned model. The code turns our training data and these tags as tokens in a mathematical array. When we then fine-tune, it is like we are dialing the focus tight into that space of language that deals with archaeological metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "c8bbc590360e4987844ee76834655d2f",
            "dececc79fbf74f698c52d7744dd291f4",
            "d9bb5f5377ba45e58d96c9f7f74198b9",
            "1b752df539924aca88ade332ecab2cb3",
            "afdf03902f4447c9a6a0cb2350bc568f",
            "328ff7152f544b09b093723f504990cc",
            "22c32828c8cd4eddb377c57fff39ad9b",
            "1c32878dc8e84edf9351e0e10ae243be",
            "15aefd3534b14c4195675fa958c88b49",
            "1472e9a1fcd74d6dada99f0063126721",
            "5c424c990fee44dcbeff293cfd714343",
            "9bbe274d0bed4745af4cca755d297602",
            "f385f377d43a4e048e9874816b110c3a",
            "5004c46927c4457bbc5856751f3e06a3",
            "b66dc7c8b617433299cdff946a2c9594",
            "797a96ec36bf411d9f8952b63df435fa",
            "38203a18f723424d8daa2df70d3e2e04",
            "9eb4fe4b30494f7caa91cf19832c4ecf",
            "422489013fa64a819c9857ec0eff81eb",
            "6edd28184b0444a58fedeb70eeb16567",
            "7e63266b986d4263bc73ec7d3ce322b9",
            "e8bbc1db33b843c6b7aa2f67ff958a6b",
            "a4645bea43854d7dbb03a24523ba1463",
            "2f3a89d04a714eb487949b477f6f3f31",
            "30a73be23d964e4d8e6553dbecdf52d2",
            "bb738fb06d424760bb3b1fa5a81dd1cb",
            "ec8b23422009492fbf8a790374a622c7",
            "699461f690fe4c2aac94c458e945e633",
            "00a3a7fb8f5c4a7a8f5aa1977693e0bd",
            "e9936561467a41a9ba337db0ab4aadbf",
            "18e862bbac064c979652f801e4a85b8a",
            "5e267b9ce54f4b878cd7fc8d8dda8837",
            "3266180417454ebba1be2ede626bdea9"
          ]
        },
        "id": "PhE1PKtmkyDp",
        "outputId": "aa871b51-c63c-41aa-b978-b64e900a1baf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8bbc590360e4987844ee76834655d2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bbe274d0bed4745af4cca755d297602",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4645bea43854d7dbb03a24523ba1463",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "def create_training_examples(data, tokenizer, label_map):\n",
        "    training_examples = []\n",
        "\n",
        "    for item in data:\n",
        "        description = item[\"description\"]\n",
        "        entities = []\n",
        "\n",
        "        # Organization\n",
        "        if \"people\" in item:\n",
        "            for person in item[\"people\"]:\n",
        "              if \"Creator:\" in person:\n",
        "                org_name = person.split(\":\")[1]\n",
        "                start = description.find(org_name)\n",
        "                if start != -1:\n",
        "                  end = start + len(org_name)\n",
        "                  entities.append({\"start\": start, \"end\": end, \"label\": \"ORG\"})\n",
        "\n",
        "        # Location\n",
        "        location_info = item.get(\"location\", {})\n",
        "        for key, value in location_info.items():\n",
        "            if isinstance(value, str):\n",
        "              start = description.find(value)\n",
        "              if start != -1:\n",
        "                end = start + len(value)\n",
        "                entities.append({\"start\": start, \"end\": end, \"label\": \"LOC\"})\n",
        "\n",
        "\n",
        "        # Periods and Subjects\n",
        "        periods = item[\"period_subject\"].get(\"periods\", [])\n",
        "        subjects = item[\"period_subject\"].get(\"subjects\", [])\n",
        "        identifiers = item.get(\"identifiers\", {})\n",
        "\n",
        "        period_text = f\" Periods: {', '.join(periods)}. \"\n",
        "        subject_text = f\"Subjects: {', '.join(subjects)}. \"\n",
        "        identifier_text = f\"Identifiers: {', '.join([f'{k}: {v}' for k,v in identifiers.items()])}.\"\n",
        "        full_text = description + period_text + subject_text + identifier_text\n",
        "\n",
        "        # Process the periods\n",
        "        period_start = len(description) + len(\" Periods: \")\n",
        "        for period in periods:\n",
        "            start = full_text.find(period, period_start)\n",
        "            if start != -1:\n",
        "                end = start + len(period)\n",
        "                entities.append({\"start\": start, \"end\": end, \"label\": \"PER\"})\n",
        "                period_start = end + len(\", \")  # Update the start for the next period\n",
        "\n",
        "        # Process the subjects\n",
        "        subject_start = len(description) + len(period_text) + len(\"Subjects: \")\n",
        "        for subject in subjects:\n",
        "            start = full_text.find(subject, subject_start)\n",
        "            if start != -1:\n",
        "                end = start + len(subject)\n",
        "                entities.append({\"start\": start, \"end\": end, \"label\": \"SUBJ\"})\n",
        "                subject_start = end + len(\", \")  # Update the start for the next subject\n",
        "\n",
        "        #Process identifiers\n",
        "        identifier_start = len(description) + len(period_text) + len(subject_text) + len(\"Identifiers: \")\n",
        "        for key, value in identifiers.items():\n",
        "            identifier_str = f\"{key}: {value}\"\n",
        "            start = full_text.find(identifier_str, identifier_start)\n",
        "            if start != -1:\n",
        "               end = start + len(identifier_str)\n",
        "               entities.append({\"start\": start, \"end\": end, \"label\": \"ID\"})\n",
        "               identifier_start = end + len(\", \") # update position\n",
        "\n",
        "\n",
        "        training_examples.append({\"text\": full_text, \"entities\": entities})\n",
        "\n",
        "    return training_examples\n",
        "\n",
        "def format_for_ner(training_examples, tokenizer, label_map, max_length, scheme=\"BIO\"):\n",
        "    \"\"\"\n",
        "    Format training examples for NER with BIO or BILOU tagging schemes\n",
        "\n",
        "    Args:\n",
        "        training_examples: List of examples with text and entities\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        label_map: Dictionary mapping entity types to IDs\n",
        "        max_length: Maximum sequence length\n",
        "        scheme: Tagging scheme, either \"BIO\" or \"BILOU\"\n",
        "    \"\"\"\n",
        "    if scheme not in [\"BIO\", \"BILOU\"]:\n",
        "        raise ValueError(\"Scheme must be either 'BIO' or 'BILOU'\")\n",
        "\n",
        "    # Expand label map for BIO or BILOU tags\n",
        "    expanded_label_map = {\"O\": 0}  # Outside tag\n",
        "    current_idx = 1\n",
        "\n",
        "    if scheme == \"BIO\":\n",
        "        for label, _ in label_map.items():\n",
        "            if label != \"O\":\n",
        "                expanded_label_map[f\"B-{label}\"] = current_idx\n",
        "                current_idx += 1\n",
        "                expanded_label_map[f\"I-{label}\"] = current_idx\n",
        "                current_idx += 1\n",
        "    else:  # BILOU\n",
        "        for label, _ in label_map.items():\n",
        "            if label != \"O\":\n",
        "                expanded_label_map[f\"B-{label}\"] = current_idx\n",
        "                current_idx += 1\n",
        "                expanded_label_map[f\"I-{label}\"] = current_idx\n",
        "                current_idx += 1\n",
        "                expanded_label_map[f\"L-{label}\"] = current_idx\n",
        "                current_idx += 1\n",
        "                expanded_label_map[f\"U-{label}\"] = current_idx\n",
        "                current_idx += 1\n",
        "\n",
        "    formatted_data = []\n",
        "    for example in training_examples:\n",
        "        text = example[\"text\"]\n",
        "        entities = example[\"entities\"]\n",
        "\n",
        "        # Sort entities by start position for proper sequential labeling\n",
        "        entities = sorted(entities, key=lambda x: x[\"start\"])\n",
        "\n",
        "        # Tokenize text and obtain token ids\n",
        "        encoding = tokenizer(text, return_offsets_mapping=True, padding=\"max_length\",\n",
        "                           truncation=True, max_length=max_length)\n",
        "        input_ids = encoding[\"input_ids\"]\n",
        "        attention_mask = encoding[\"attention_mask\"]\n",
        "        offset_mapping = encoding[\"offset_mapping\"]\n",
        "        labels = [0] * len(input_ids)  # Initialize all labels as O\n",
        "\n",
        "        # Process each entity\n",
        "        for entity in entities:\n",
        "            start_char = entity[\"start\"]\n",
        "            end_char = entity[\"end\"]\n",
        "            label_name = entity[\"label\"]\n",
        "\n",
        "            # Find all tokens that overlap with the entity\n",
        "            entity_tokens = []\n",
        "            for idx, (offset_start, offset_end) in enumerate(offset_mapping):\n",
        "                if offset_start is None or offset_end is None:  # Skip special tokens\n",
        "                    continue\n",
        "                if offset_start <= end_char and offset_end > start_char:\n",
        "                    entity_tokens.append(idx)\n",
        "\n",
        "            if not entity_tokens:  # Skip if no tokens found for entity\n",
        "                continue\n",
        "\n",
        "            # Assign labels based on scheme\n",
        "            if scheme == \"BIO\":\n",
        "                for i, token_idx in enumerate(entity_tokens):\n",
        "                    if i == 0:  # First token\n",
        "                        labels[token_idx] = expanded_label_map[f\"B-{label_name}\"]\n",
        "                    else:  # Subsequent tokens\n",
        "                        labels[token_idx] = expanded_label_map[f\"I-{label_name}\"]\n",
        "            else:  # BILOU scheme\n",
        "                if len(entity_tokens) == 1:  # Unit-length entity\n",
        "                    labels[entity_tokens[0]] = expanded_label_map[f\"U-{label_name}\"]\n",
        "                else:\n",
        "                    for i, token_idx in enumerate(entity_tokens):\n",
        "                        if i == 0:  # Beginning\n",
        "                            labels[token_idx] = expanded_label_map[f\"B-{label_name}\"]\n",
        "                        elif i == len(entity_tokens) - 1:  # Last\n",
        "                            labels[token_idx] = expanded_label_map[f\"L-{label_name}\"]\n",
        "                        else:  # Inside\n",
        "                            labels[token_idx] = expanded_label_map[f\"I-{label_name}\"]\n",
        "\n",
        "        formatted_data.append({\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        })\n",
        "\n",
        "    return formatted_data, expanded_label_map\n",
        "\n",
        " # Load JSON data\n",
        "with open(\"processed_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# do it\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "\n",
        "# Original label map\n",
        "label_map = {\"O\": 0, \"ORG\": 1, \"LOC\": 2, \"PER\": 3, \"SUBJ\": 4, \"ID\": 5}\n",
        "\n",
        "# Create training examples (your existing code)\n",
        "training_examples = create_training_examples(data, tokenizer, label_map)\n",
        "\n",
        "# Format with BIO scheme\n",
        "formatted_data_bio, bio_label_map = format_for_ner(\n",
        "    training_examples,\n",
        "    tokenizer,\n",
        "    label_map,\n",
        "    max_length=512,\n",
        "    scheme=\"BIO\"\n",
        ")\n",
        "\n",
        "# Or format with BILOU scheme\n",
        "formatted_data_bilou, bilou_label_map = format_for_ner(\n",
        "    training_examples,\n",
        "    tokenizer,\n",
        "    label_map,\n",
        "    max_length=512,\n",
        "    scheme=\"BILOU\"\n",
        ")\n",
        "\n",
        "# Save the formatted data and label maps\n",
        "np.save(\"input_ids.npy\", np.array([d['input_ids'] for d in formatted_data_bio]))\n",
        "np.save(\"attention_mask.npy\", np.array([d['attention_mask'] for d in formatted_data_bio]))\n",
        "np.save(\"labels.npy\", np.array([d['labels'] for d in formatted_data_bio]))\n",
        "\n",
        "# Save label maps for use during inference\n",
        "with open(\"label_map.json\", \"w\") as f:\n",
        "    json.dump(bio_label_map, f)  # or bilou_label_map if using BILOU scheme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7LZCTQFeQYf"
      },
      "source": [
        "Now we're ready to fine tune, once we install one last piece of code we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAM62XQDmgx2"
      },
      "outputs": [],
      "source": [
        "# gotta use this right now since the release of modernbert hasn't been\n",
        "# updated in the main transformers yet, apparently\n",
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "#restart session then continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kG0ogScdGRQo",
        "outputId": "895d7845-b126-4f3b-ab96-f17532fa5af4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1573: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-2-faa5ac6aabd8>:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='551' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 551/1050 20:33 < 18:41, 0.44 it/s, Epoch 25/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.612500</td>\n",
              "      <td>1.609851</td>\n",
              "      <td>0.627677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.761900</td>\n",
              "      <td>0.490151</td>\n",
              "      <td>0.847943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.573800</td>\n",
              "      <td>0.139436</td>\n",
              "      <td>0.962247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.166800</td>\n",
              "      <td>0.057510</td>\n",
              "      <td>0.982353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.080500</td>\n",
              "      <td>0.038729</td>\n",
              "      <td>0.987293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.051800</td>\n",
              "      <td>0.032361</td>\n",
              "      <td>0.988764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.040100</td>\n",
              "      <td>0.030121</td>\n",
              "      <td>0.989177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.030800</td>\n",
              "      <td>0.029905</td>\n",
              "      <td>0.989786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.023900</td>\n",
              "      <td>0.030070</td>\n",
              "      <td>0.990016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.015500</td>\n",
              "      <td>0.033460</td>\n",
              "      <td>0.990039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.009600</td>\n",
              "      <td>0.034380</td>\n",
              "      <td>0.990257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1050/1050 41:52, Epoch 47/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.612500</td>\n",
              "      <td>1.609851</td>\n",
              "      <td>0.627677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.761900</td>\n",
              "      <td>0.490151</td>\n",
              "      <td>0.847943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.573800</td>\n",
              "      <td>0.139436</td>\n",
              "      <td>0.962247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.166800</td>\n",
              "      <td>0.057510</td>\n",
              "      <td>0.982353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.080500</td>\n",
              "      <td>0.038729</td>\n",
              "      <td>0.987293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.051800</td>\n",
              "      <td>0.032361</td>\n",
              "      <td>0.988764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.040100</td>\n",
              "      <td>0.030121</td>\n",
              "      <td>0.989177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.030800</td>\n",
              "      <td>0.029905</td>\n",
              "      <td>0.989786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.023900</td>\n",
              "      <td>0.030070</td>\n",
              "      <td>0.990016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.015500</td>\n",
              "      <td>0.033460</td>\n",
              "      <td>0.990039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.009600</td>\n",
              "      <td>0.034380</td>\n",
              "      <td>0.990257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.038215</td>\n",
              "      <td>0.990384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.039971</td>\n",
              "      <td>0.990269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.043592</td>\n",
              "      <td>0.990119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.045018</td>\n",
              "      <td>0.990177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.046760</td>\n",
              "      <td>0.990165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.048043</td>\n",
              "      <td>0.990269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.049045</td>\n",
              "      <td>0.990246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.049694</td>\n",
              "      <td>0.990188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.050110</td>\n",
              "      <td>0.990280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.050200</td>\n",
              "      <td>0.990246</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning completed and model saved.\n"
          ]
        }
      ],
      "source": [
        "### new finetuning\n",
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "# 1. Load the label map and determine number of labels\n",
        "with open(\"label_map.json\", \"r\") as f:\n",
        "    label_map = json.load(f)\n",
        "num_labels = len(label_map)  # This will be 11 for BIO scheme with 5 entity types + O\n",
        "\n",
        "# 2. Load the pre-trained Model and Tokenizer\n",
        "model_name = \"answerdotai/ModernBERT-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    id2label={str(i): label for label, i in label_map.items()},\n",
        "    label2id={label: i for label, i in label_map.items()}\n",
        ")\n",
        "\n",
        "# 3. Load and Format the Data\n",
        "max_length = 512\n",
        "input_ids = np.load(\"input_ids.npy\", allow_pickle=True)\n",
        "attention_mask = np.load(\"attention_mask.npy\", allow_pickle=True)\n",
        "labels = np.load(\"labels.npy\", allow_pickle=True)\n",
        "\n",
        "# Split Data for training and validation\n",
        "input_ids_train, input_ids_val, attention_mask_train, attention_mask_val, labels_train, labels_val = train_test_split(\n",
        "    input_ids, attention_mask, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": input_ids_train,\n",
        "    \"attention_mask\": attention_mask_train,\n",
        "    \"labels\": labels_train\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": input_ids_val,\n",
        "    \"attention_mask\": attention_mask_val,\n",
        "    \"labels\": labels_val\n",
        "})\n",
        "\n",
        "# 4. Define custom compute_metrics function for NER evaluation\n",
        "\n",
        "# Create inverse label mapping\n",
        "label_map_inverse = {i: label for label, i in label_map.items()}\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Only evaluate on non-padded tokens\n",
        "    true_predictions = [\n",
        "        [label_map_inverse[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_map_inverse[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # Calculate accuracy for non-padded tokens\n",
        "    correct = sum(p == l for pred, lab in zip(true_predictions, true_labels)\n",
        "                 for p, l in zip(pred, lab))\n",
        "    total = sum(len(pred) for pred in true_predictions)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "\n",
        "# 5. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_ner_bio\",          # Updated output directory\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,       # Increased batch size if memory allows\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    logging_dir=\"./logs\",                 # Add logging\n",
        "    logging_steps=50,\n",
        "    fp16=True,                           # Enable mixed precision training if available\n",
        "    gradient_accumulation_steps=2,        # Accumulate gradients for larger effective batch size\n",
        "    warmup_steps=500,                    # Add warmup steps\n",
        "    seed=42,                             # Set random seed for reproducibility\n",
        "    report_to=\"none\",                    # without this, colab logs the run with 'weights and biases' service, which requires an api etc\n",
        ")\n",
        "\n",
        "# 6. Define and Start Training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics      # Add metrics computation\n",
        ")\n",
        "\n",
        "# 7. Train the Model\n",
        "trainer.train()\n",
        "\n",
        "# 8. Save the model and tokenizer\n",
        "trainer.save_model(\"./bert_arch_ner_bio_trained\")\n",
        "tokenizer.save_pretrained(\"./bert_arch_ner_bio_trained\")\n",
        "\n",
        "# 9. Save the label map with the model for inference\n",
        "with open(\"./bert_arch_ner_bio_trained/label_map.json\", \"w\") as f:\n",
        "    json.dump(label_map, f, indent=2)\n",
        "\n",
        "print(\"Fine-tuning completed and model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB7C37xReZNX"
      },
      "source": [
        "And now we can test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgtmiqk6sl3U",
        "outputId": "46a3cbd8-d928-4811-923b-415a9188cdc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input Text:\n",
            "This archive presents appendices B-I and supplementary material resulting from the programme of archaeological works undertaken during the construction scheme to widen the A1 trunk road between Dishforth and Leeming Bar in North Yorkshire. The Iron Age to early medieval evidence from Healam Bridge, along with other evidence for Roman activity along the route is published in two volumes\n",
            "\n",
            "Extracted Entities:\n",
            "\n",
            "Input Text:\n",
            "This collection comprises images, spreadsheets, reports, vector graphics, and scanned site records and drawings from archaeological recording by Archaeological Research Services at Lower Radbourne Deserted Medieval Village, Warwickshire. The work was undertaken between April and December 2021. Area C32070 was dominated by intercutting features predominantly dated to two broad phases, prehistoric and medieval. The prehistoric features were represented by a large ring ditch, potentially dating to the Early Bronze Age, four smaller potential Bronze Age ring ditches and a series of intercutting drip gullies, probably Iron Age in date.\n",
            "\n",
            "Extracted Entities:\n",
            "  - Text:  Radbourne Deserted, Start: 186, End: 205, Label: LOC\n",
            "  - Text:  Medieval Village,, Start: 205, End: 223, Label: LOC\n",
            "  - Text:  Warwickshire., Start: 223, End: 237, Label: LOC\n",
            "\n",
            "Input Text:\n",
            "This collection comprises images and CAD from an archaeological evaluation and watching brief, undertaken by Cotswold Archaeology in August 2018, at Hewmar House, 120 London Road, Gloucester, Gloucestershire. Four archaeological evaluation trenches were excavated and four geotechnical test pits were  observed. Despite the proximity of the site to Wotton Roman cemetery, no evidence for any in situ burials, or indeed any Roman activity, was identified in any of the excavated trenches or test pits. It is likely that the site lay beyond the southern boundary of the cemetery and formed  part of the agricultural hinterland of both Roman and medieval Gloucester until the  construction of Hillfield Villa (later Hewmar House) in the early 19th century. Three linear  garden features, probably planting trenches, associated with Hillfield Villa and a large undated ditch were identified. Evidence for possible quarrying was also identified throughout the site. Periods: POST MEDIEVAL, 1800 - 1850, UNCERTAIN. Subjects: Archaeology, Evaluation, DITCH, GARDEN FEATURE, TRIAL TRENCH.\n",
            "\n",
            "Extracted Entities:\n",
            "  - Text:  Cotswold Archaeology in, Start: 108, End: 132, Label: ORG\n",
            "  - Text:  London Road, Start: 166, End: 178, Label: LOC\n",
            "  - Text:  Gloucester, Start: 179, End: 190, Label: LOC\n",
            "  - Text:  Gloucestershire., Start: 191, End: 208, Label: LOC\n",
            "  - Text:  POST MEDIEVAL,, Start: 969, End: 984, Label: PER\n",
            "  - Text:  1800 - 1850,, Start: 984, End: 997, Label: PER\n",
            "  - Text:  UNCERTAIN., Start: 997, End: 1008, Label: PER\n",
            "  - Text:  Archaeology,, Start: 1018, End: 1031, Label: SUBJ\n",
            "  - Text:  Evaluation,, Start: 1031, End: 1043, Label: SUBJ\n",
            "  - Text:  DITCH,, Start: 1043, End: 1050, Label: SUBJ\n",
            "  - Text:  GARDEN FEATURE,, Start: 1050, End: 1066, Label: SUBJ\n",
            "  - Text:  TRIAL TRENCH., Start: 1066, End: 1080, Label: SUBJ\n"
          ]
        }
      ],
      "source": [
        "#test it out!\n",
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the Trained Model and Tokenizer\n",
        "model_name = \"./bert_arch_ner_bio_trained\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# we need the label map\n",
        "label_map = {\n",
        "    \"O\": 0,\n",
        "    \"B-ORG\": 1, \"I-ORG\": 2,\n",
        "    \"B-LOC\": 3, \"I-LOC\": 4,\n",
        "    \"B-PER\": 5, \"I-PER\": 6,\n",
        "    \"B-SUBJ\": 7, \"I-SUBJ\": 8,\n",
        "    \"B-ID\": 9, \"I-ID\": 10\n",
        "}\n",
        "id_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "def predict_entities(text, tokenizer, model, label_map, max_length):\n",
        "    inputs = tokenizer(text, return_offsets_mapping=True, padding=\"max_length\",\n",
        "                      truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    offset_mapping = inputs[\"offset_mapping\"].squeeze().tolist()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, axis=-1).squeeze().tolist()\n",
        "\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    entity_tokens = []\n",
        "\n",
        "    for idx, label_id in enumerate(predictions):\n",
        "        offset_start, offset_end = offset_mapping[idx]\n",
        "        if offset_start is None or offset_end is None:  # Skip special tokens\n",
        "            continue\n",
        "\n",
        "        label = id_to_label[label_id]\n",
        "\n",
        "        if label.startswith('B-'):  # Beginning of new entity\n",
        "            if current_entity:  # Save previous entity if exists\n",
        "                start = entity_tokens[0][0]\n",
        "                end = entity_tokens[-1][1]\n",
        "                entities.append({\n",
        "                    \"text\": text[start:end],\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"label\": current_entity.replace('B-', '')\n",
        "                })\n",
        "            current_entity = label\n",
        "            entity_tokens = [(offset_start, offset_end)]\n",
        "\n",
        "        elif label.startswith('I-'):  # Inside of entity\n",
        "            if current_entity and label[2:] == current_entity[2:]:  # Matches current entity type\n",
        "                entity_tokens.append((offset_start, offset_end))\n",
        "            else:  # 'I-' without matching 'B-' - ignore\n",
        "                current_entity = None\n",
        "                entity_tokens = []\n",
        "\n",
        "        else:  # 'O' or any other label\n",
        "            if current_entity:  # Save previous entity if exists\n",
        "                start = entity_tokens[0][0]\n",
        "                end = entity_tokens[-1][1]\n",
        "                entities.append({\n",
        "                    \"text\": text[start:end],\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"label\": current_entity.replace('B-', '')\n",
        "                })\n",
        "            current_entity = None\n",
        "            entity_tokens = []\n",
        "\n",
        "    # Handle last entity if exists\n",
        "    if current_entity and entity_tokens:\n",
        "        start = entity_tokens[0][0]\n",
        "        end = entity_tokens[-1][1]\n",
        "        entities.append({\n",
        "            \"text\": text[start:end],\n",
        "            \"start\": start,\n",
        "            \"end\": end,\n",
        "            \"label\": current_entity.replace('B-', '')\n",
        "        })\n",
        "\n",
        "    return entities\n",
        "\n",
        "# 3. Input Text and Run Inference\n",
        "test_texts = [\n",
        "    \"This archive presents appendices B-I and supplementary material resulting from the programme of archaeological works undertaken during the construction scheme to widen the A1 trunk road between Dishforth and Leeming Bar in North Yorkshire. The Iron Age to early medieval evidence from Healam Bridge, along with other evidence for Roman activity along the route is published in two volumes\",\n",
        "    \"This collection comprises images, spreadsheets, reports, vector graphics, and scanned site records and drawings from archaeological recording by Archaeological Research Services at Lower Radbourne Deserted Medieval Village, Warwickshire. The work was undertaken between April and December 2021. Area C32070 was dominated by intercutting features predominantly dated to two broad phases, prehistoric and medieval. The prehistoric features were represented by a large ring ditch, potentially dating to the Early Bronze Age, four smaller potential Bronze Age ring ditches and a series of intercutting drip gullies, probably Iron Age in date.\",\n",
        "    \"This collection comprises images and CAD from an archaeological evaluation and watching brief, undertaken by Cotswold Archaeology in August 2018, at Hewmar House, 120 London Road, Gloucester, Gloucestershire. Four archaeological evaluation trenches were excavated and four geotechnical test pits were  observed. Despite the proximity of the site to Wotton Roman cemetery, no evidence for any in situ burials, or indeed any Roman activity, was identified in any of the excavated trenches or test pits. It is likely that the site lay beyond the southern boundary of the cemetery and formed  part of the agricultural hinterland of both Roman and medieval Gloucester until the  construction of Hillfield Villa (later Hewmar House) in the early 19th century. Three linear  garden features, probably planting trenches, associated with Hillfield Villa and a large undated ditch were identified. Evidence for possible quarrying was also identified throughout the site. Periods: POST MEDIEVAL, 1800 - 1850, UNCERTAIN. Subjects: Archaeology, Evaluation, DITCH, GARDEN FEATURE, TRIAL TRENCH.\"\n",
        "    ]\n",
        "\n",
        "\n",
        "max_length = 512\n",
        "for text in test_texts:\n",
        "    predicted_entities = predict_entities(text, tokenizer, model, label_map, max_length)\n",
        "\n",
        "    print(\"\\nInput Text:\")\n",
        "    print(text)\n",
        "    print(\"\\nExtracted Entities:\")\n",
        "    for entity in predicted_entities:\n",
        "        print(f\"  - Text: {entity['text']}, Start: {entity['start']}, End: {entity['end']}, Label: {entity['label']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xvLWgIemIo"
      },
      "source": [
        "Save your fine tuned ModernBERT model to huggingface; change `your-username` accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "bf6b6d07db9a45ae96480f9bb92b7c8f",
            "801625f015bb45d19e3a8609f4784ef8",
            "0ea50dce42654b9bbd38a743c739a73f",
            "70351d79b25c4d628a2c5519c1783138",
            "9e0b34c7127745748a532c8df9884c75",
            "c370e8558dd6406d85d257375b7a5d6a",
            "9309f72c04e24fc4a7fad1052e9da172",
            "c103018c084a4596ac88048ecff194d8",
            "a4b97266c1ed4326895fcb62b4c9edbf",
            "07f6092dbb984b30bbf6b4c8c72d086f",
            "0736903ce0064d9a8a626e5486bc5cb1",
            "29d34988ce7c473489ea4dbf36833bee",
            "9303000043874a7cbe360186a84b5b9a",
            "934927be67974e50ba0cebbc032d7c8f",
            "ad0567a49dd74934ba2b12d8b8ebae60",
            "843a93197ed746b2a580d9601060e28b",
            "8ec995fb280a4bd1877e812c1d3b7f01",
            "de5e0c1685e648c1a29ba8ae7d43489b",
            "e83ae5b5947c4f46886de0039990e4c6",
            "963b012362a14e81a2e9311ed5f2bb30",
            "28f58f619cf943c389899145a26e5d43",
            "14173d01041c4086b6c30d08cd72df0d",
            "25f989235ec54a8693154cbc10f404d2",
            "2481f6212f124258b0d02d09ef5924c4",
            "c8fd2b82dc7f43a5b110d5ce317ce639",
            "c203ddd8bc4747b2bf65529fd1daf54a",
            "2d73ef76fe0640bb8684319eff2359e1",
            "7359cf9a7a8949799e2dff7cf0325c9b",
            "9fd6c1d54dcf47229f2b4ff659542de7",
            "871a337a95b041b5aa0d7e0479a2edbf",
            "6195c72da67a4e37a2f0b613d9b510cb",
            "8c0192f8c61a4301a3862abea4e3be64",
            "966478a7fe974baf8158760e3f530f5d",
            "ba59b7f4936f4e0ab6f27cc986fb0357",
            "f4fae1335e5d402c98ea8f63b5035f38",
            "d38d3f8be59142e5aba6a8a5e577b7f8",
            "f1c5453a04e243f6bc0f609b538debc7",
            "b04edc41f1b446df86c103c28c73113b",
            "478f094b3fb94fe886834ccba7ef3b5e"
          ]
        },
        "id": "FxSPiuWBynqp",
        "outputId": "2ea13b89-d8ff-462f-a530-e14c39254d9b"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# First, login to Hugging Face\n",
        "notebook_login()\n",
        "\n",
        "# Save the model and all necessary files\n",
        "trainer.save_model(\"./bert_arch_ner_bio_trained\")\n",
        "\n",
        "# Save the tokenizer and label map with the model\n",
        "tokenizer.save_pretrained(\"./bert_arch_ner_bio_trained\")\n",
        "with open(\"./bert_arch_ner_bio_trained/label_map.json\", \"w\") as f:\n",
        "    json.dump(label_map, f, indent=2)\n",
        "\n",
        "# Push to hub with a model card\n",
        "model.push_to_hub(\"your-username/ModernBERT_archae\",\n",
        "    use_auth_token=True,\n",
        "    model_card_kwargs={\n",
        "        \"language\": \"en\",\n",
        "        \"license\": \"mit\",\n",
        "        \"tags\": [\"token-classification\", \"ner\", \"archaeology\"],\n",
        "        \"datasets\": [\"custom archaeology dataset\"],\n",
        "        \"metrics\": [\"accuracy\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Push the tokenizer configuration\n",
        "tokenizer.push_to_hub(\"your-username/ModernBERT_archae\")\n",
        "\n",
        "# Initialize the Hugging Face API\n",
        "api = HfApi()\n",
        "\n",
        "# Push the label map as a separate file\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"./bert_arch_ner_bio_trained/label_map.json\",\n",
        "    path_in_repo=\"label_map.json\",\n",
        "    repo_id=\"your-username/ModernBERT_archae\",\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE9TSjI31tTC"
      },
      "source": [
        "example of use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70RC9A1h9adz",
        "outputId": "bab36907-2c09-4e4f-d27c-d00f55328420"
      },
      "outputs": [],
      "source": [
        "## get some test data, both in json and csv, both from ADS search results again.\n",
        "# csv where all of the fields have been smooshed into a single column\n",
        "!wget https://gist.githubusercontent.com/shawngraham/15c7cf3e2982d645b0c03c745f12e6bf/raw/b06b6333aa14dd7d40bb14aac79b3434db3afdd0/test.csv\n",
        "# json where everything smooshed into description field\n",
        "!wget https://gist.githubusercontent.com/shawngraham/15c7cf3e2982d645b0c03c745f12e6bf/raw/fda5ebd3a9674f4ba1546d27a118938000034d2d/test.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6KYFowYfBRP"
      },
      "source": [
        "...remember to change 'your-username' as appropriate below. Notice that we're still using the base ModernBERT to handle basic transformations (base_tokenizer), and our own saved model for the particular task we've trained for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SdpIkgy1wFA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "class ArchaeologyNERProcessor:\n",
        "    def __init__(self, model_name=\"your-username/ModernBERT_archae\", base_tokenizer=\"answerdotai/ModernBERT-base\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_tokenizer)\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "        # Define label map with BIO scheme\n",
        "        self.label_map = {\n",
        "            \"O\": 0,\n",
        "            \"B-ORG\": 1, \"I-ORG\": 2,\n",
        "            \"B-LOC\": 3, \"I-LOC\": 4,\n",
        "            \"B-PER\": 5, \"I-PER\": 6,\n",
        "            \"B-SUBJ\": 7, \"I-SUBJ\": 8,\n",
        "            \"B-ID\": 9, \"I-ID\": 10\n",
        "        }\n",
        "        self.id_to_label = {v: k for k, v in self.label_map.items()}\n",
        "        self.max_length = 512\n",
        "\n",
        "    def predict_entities(self, text):\n",
        "        inputs = tokenizer = self.tokenizer(\n",
        "            text,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=inputs[\"input_ids\"],\n",
        "                               attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "        predictions = torch.argmax(outputs.logits, axis=-1).squeeze().tolist()\n",
        "        offset_mapping = inputs[\"offset_mapping\"].squeeze().tolist()\n",
        "\n",
        "        entities = []\n",
        "        current_entity = None\n",
        "        entity_tokens = []\n",
        "\n",
        "        for idx, label_id in enumerate(predictions):\n",
        "            offset_start, offset_end = offset_mapping[idx]\n",
        "            if offset_start is None or offset_end is None:\n",
        "                continue\n",
        "\n",
        "            label = self.id_to_label[label_id]\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                if current_entity:\n",
        "                    start = entity_tokens[0][0]\n",
        "                    end = entity_tokens[-1][1]\n",
        "                    entities.append({\n",
        "                        \"text\": text[start:end],\n",
        "                        \"start\": start,\n",
        "                        \"end\": end,\n",
        "                        \"label\": current_entity.replace('B-', '')\n",
        "                    })\n",
        "                current_entity = label\n",
        "                entity_tokens = [(offset_start, offset_end)]\n",
        "\n",
        "            elif label.startswith('I-'):\n",
        "                if current_entity and label[2:] == current_entity[2:]:\n",
        "                    entity_tokens.append((offset_start, offset_end))\n",
        "                else:\n",
        "                    current_entity = None\n",
        "                    entity_tokens = []\n",
        "\n",
        "            else:  # 'O' or any other label\n",
        "                if current_entity:\n",
        "                    start = entity_tokens[0][0]\n",
        "                    end = entity_tokens[-1][1]\n",
        "                    entities.append({\n",
        "                        \"text\": text[start:end],\n",
        "                        \"start\": start,\n",
        "                        \"end\": end,\n",
        "                        \"label\": current_entity.replace('B-', '')\n",
        "                    })\n",
        "                current_entity = None\n",
        "                entity_tokens = []\n",
        "\n",
        "        if current_entity and entity_tokens:\n",
        "            start = entity_tokens[0][0]\n",
        "            end = entity_tokens[-1][1]\n",
        "            entities.append({\n",
        "                \"text\": text[start:end],\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "                \"label\": current_entity.replace('B-', '')\n",
        "            })\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def process_file(self, input_path, output_path=None, text_column=\"description\"):\n",
        "        \"\"\"Process input file (JSON or CSV) and extract entities\"\"\"\n",
        "        input_path = Path(input_path)\n",
        "\n",
        "        # Determine input file type\n",
        "        if input_path.suffix.lower() == '.json':\n",
        "            with open(input_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, dict):\n",
        "                    data = [data]\n",
        "\n",
        "        elif input_path.suffix.lower() == '.csv':\n",
        "            data = pd.read_csv(input_path).to_dict('records')\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type. Please use .json or .csv\")\n",
        "\n",
        "        # Process each record\n",
        "        output_data = []\n",
        "        for record in data:\n",
        "            if text_column not in record:\n",
        "                print(f\"Warning: '{text_column}' column not found in record, skipping...\")\n",
        "                continue\n",
        "\n",
        "            text = record[text_column]\n",
        "            extracted_entities = self.predict_entities(text)\n",
        "            record[\"extracted_entities\"] = extracted_entities\n",
        "            output_data.append(record)\n",
        "\n",
        "        # Determine output path if not provided\n",
        "        if output_path is None:\n",
        "            timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "            output_path = input_path.parent / f\"processed_{timestamp}{input_path.suffix}\"\n",
        "        else:\n",
        "            output_path = Path(output_path)\n",
        "\n",
        "        # Save processed data\n",
        "        if output_path.suffix.lower() == '.json':\n",
        "            with open(output_path, 'w') as f:\n",
        "                json.dump(output_data, f, indent=2)\n",
        "        elif output_path.suffix.lower() == '.csv':\n",
        "            # Flatten the entities for CSV output\n",
        "            flattened_data = []\n",
        "            for record in output_data:\n",
        "                base_record = {k: v for k, v in record.items() if k != \"extracted_entities\"}\n",
        "                if record[\"extracted_entities\"]:\n",
        "                    for entity in record[\"extracted_entities\"]:\n",
        "                        new_record = base_record.copy()\n",
        "                        new_record.update({\n",
        "                            \"entity_text\": entity[\"text\"],\n",
        "                            \"entity_start\": entity[\"start\"],\n",
        "                            \"entity_end\": entity[\"end\"],\n",
        "                            \"entity_label\": entity[\"label\"]\n",
        "                        })\n",
        "                        flattened_data.append(new_record)\n",
        "                else:\n",
        "                    base_record.update({\n",
        "                        \"entity_text\": \"\",\n",
        "                        \"entity_start\": \"\",\n",
        "                        \"entity_end\": \"\",\n",
        "                        \"entity_label\": \"\"\n",
        "                    })\n",
        "                    flattened_data.append(base_record)\n",
        "\n",
        "            pd.DataFrame(flattened_data).to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"Processing complete. Output saved to: {output_path}\")\n",
        "        return output_data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvp4T56z9nFV",
        "outputId": "483ba6da-1d23-454f-fa29-e2f58382eb7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing complete. Output saved to: output.json\n",
            "Processing complete. Output saved to: output.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'description': 'Report from Old Quarry Field at Emberton \"This report documents archaeological findings from a survey led by Granite Digs at Hidden Valley located near Oakhaven. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Ironmill Greenfield Emberton England 21483571 27700:390648 Subject:Archaeology Subject:Sherd Period:-800 - 1800 Subject:Excavations (Archaeology)--England Subject:Ditch Subject:Pit Subject:Strip Map And Sample Subject:Field Observation (Monitoring) Subject:Excavations (Archaeology)--England Period:ROMAN Associated ID: FAKE3 Import RCN: B45-546542 Creator:Starlight Research',\n",
              "  'extracted_entities': [{'text': 'Sher',\n",
              "    'start': 352,\n",
              "    'end': 356,\n",
              "    'label': 'SUBJ'},\n",
              "   {'text': ' Import RCN: B45-546542 Creator:Star',\n",
              "    'start': 590,\n",
              "    'end': 626,\n",
              "    'label': 'ID'}]},\n",
              " {'description': 'Report from Hidden Valley at Silverton \"This report documents archaeological findings from a survey led by Granite Digs at Lost Temple located near Emberton. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Oakhaven Oakhaven Silverton England 36302597 27700:834424 Subject:Archaeology Subject:Gully Subject:Field Drain Subject:Farmstead  Period:LATE IRON AGE     Associated ID: FAKE4 GHI-789-RST Creator:Starlight Research',\n",
              "  'extracted_entities': []},\n",
              " {'description': 'Report from Old Quarry Field at Stonebridge \"This report documents archaeological findings from a survey led by Silverline Investigations at Old Quarry Field located near Oakhaven. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Oakhaven Emberton Silverton England 89146727 27700:858119  Subject:TRIAL TRENCH Subject:Evaluation   Period:1800 - 1850 Subject:Archaeology    STU-890-PQR Associated ID: FAKE3 Creator:Crimson Heritage',\n",
              "  'extracted_entities': [{'text': ' Stone',\n",
              "    'start': 31,\n",
              "    'end': 37,\n",
              "    'label': 'LOC'},\n",
              "   {'text': ' Oak', 'start': 283, 'end': 287, 'label': 'LOC'},\n",
              "   {'text': '  ', 'start': 341, 'end': 343, 'label': 'PER'},\n",
              "   {'text': 'Evaluation   ', 'start': 372, 'end': 385, 'label': 'ID'}]},\n",
              " {'description': 'Report from Lost Temple at Stonebridge \"This report documents archaeological findings from a survey led by Obsidian Finds at Eagle Ridge located near Goldengrove. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Emberton Oakhaven Ashwood England 93954876 27700:208663 Period:UNCERTAIN     Subject:Tile Subject:Excavations (Archaeology)--England Subject:Ridge And Furrow   STU-890-PQR ABC-123-XYZ Creator:Ironclad Surveyors',\n",
              "  'extracted_entities': [{'text': ' Stone',\n",
              "    'start': 26,\n",
              "    'end': 32,\n",
              "    'label': 'LOC'}]},\n",
              " {'description': 'Report from Lost Temple at Goldengrove \"This report documents archaeological findings from a survey led by Granite Digs at Ancient Barrows located near Emberton. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Ironmill Greenfield Oakhaven England 29864658 27700:235064 Period:MEDIEVAL     Period:ROMAN     DOI: 10.1111/1111111 GHI-789-RST Creator:Emerald Archaeology',\n",
              "  'extracted_entities': []},\n",
              " {'description': 'Report from Eagle Ridge at Ironmill \"This report documents archaeological findings from a survey led by Bronze Age Discoveries at Ancient Barrows located near Oakhaven. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Ironmill Ashwood Emberton England 22396825 27700:248770 Subject:Excavations (Archaeology)--England Period:ROMAN Subject:Carpet Factory   Period:POST MEDIEVAL Period:43 - 2000 Subject:Carpet Factory Subject:Hand Cart Rail System Subject:Archaeology PQR-345-STU UVW-901-DEF Creator:Emerald Archaeology',\n",
              "  'extracted_entities': [{'text': 'ROMAN',\n",
              "    'start': 378,\n",
              "    'end': 383,\n",
              "    'label': 'PER'},\n",
              "   {'text': 'Carpet Factory', 'start': 455, 'end': 469, 'label': 'SUBJ'}]},\n",
              " {'description': 'Report from Crystal Caves at Silverton \"This report documents archaeological findings from a survey led by Granite Digs at Whispering Hollows located near Oakhaven. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Ashwood Emberton Stonebridge England 25568830 27700:602593 Subject:Ditch Subject:Pit    Period:43 - 410 Subject:Evaluation    Import RCN: A23-234232 GHI-789-RST Creator:Silverline Investigations',\n",
              "  'extracted_entities': [{'text': 'Import RCN: A23-234232 GHI-789-RST Creator:Silverline Investigations',\n",
              "    'start': 394,\n",
              "    'end': 462,\n",
              "    'label': 'ID'}]},\n",
              " {'description': 'Report from Lost Temple at Ironmill \"This report documents archaeological findings from a survey led by Emerald Archaeology at Lost Temple located near Blackwood. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Oakhaven Oakhaven Riverbend England 77932353 27700:434952 Subject:Evaluation Period:-2500 - 410 Subject:Settlement    Period:ROMAN    DEF-456-UVW RST-234-GHI Creator:Emerald Archaeology',\n",
              "  'extracted_entities': [{'text': ' Emer',\n",
              "    'start': 103,\n",
              "    'end': 108,\n",
              "    'label': 'ORG'}]},\n",
              " {'description': 'Report from Old Quarry Field at Emberton \"This report documents archaeological findings from a survey led by Bronze Age Discoveries at Ancient Barrows located near Stonebridge. Key results include pottery analysis  details of a potential burial ground and a system of old roads.\" Ashwood Ironmill Ashwood England 57644989 27700:168034 Subject:Ditched Enclosure Period:-1100 - 1800 Period:LATER PREHISTORIC   Subject:Field System Period:ROMAN Subject:Evaluation   RST-234-GHI MNO-567-JKL Creator:Golden Era Studies',\n",
              "  'extracted_entities': [{'text': '89',\n",
              "    'start': 319,\n",
              "    'end': 321,\n",
              "    'label': 'PER'},\n",
              "   {'text': 'Field', 'start': 416, 'end': 421, 'label': 'SUBJ'},\n",
              "   {'text': ' System', 'start': 421, 'end': 428, 'label': 'ID'}]}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the processor\n",
        "processor = ArchaeologyNERProcessor()\n",
        "\n",
        "# Process a JSON file\n",
        "processor.process_file(\"test.json\", \"output.json\")\n",
        "\n",
        "# Process a CSV file\n",
        "processor.process_file(\n",
        "    \"test.csv\",\n",
        "    \"output.csv\",\n",
        "    text_column=\"description\"  # specify which column contains the text to analyze\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}